---
title: "Assignment 2"
author: "Group 4"
date: "2024-09-16"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r imports, results='hide', message=FALSE}
options(contrasts = c("contr.sum", "contr.poly"))
require("ggplot2")
require("dplyr")
require("ppcor")
require("caret")
require("tidyr")
require("stringr")
require("lubridate")
require("tsibble")
require("ggfortify")
require("gridExtra")
require("reshape2")

library(imputeTS)   # Time series missing value imputation

library(jsonlite)   # handle JSON data returned by Frost
library(tidyr)      # unpack data from JSON format
library(tidyverse)  # data manipulation with mutate etc, string formatting
library(lubridate)  # process date and time information
library(tsibble)    # special tibbles for time series
library(fpp3)       # autoplot() and gg_season() for time series
library(readr)      # to read the Frost client ID from file
```

# Task 1: Dimension reduction on air quality data

## Part A: Get

-   Obtain data from <https://archive.ics.uci.edu/dataset/360/air+quality>.
-   Provide a brief description of the data based on the information from the website.

```{r}
airquality <- read.table("AirQualityUCI.csv", sep=";", dec=",", header= T)

airqual <- airquality %>%
  dplyr::select(-c("X", "X.1")) %>%
  na.omit() %>%
  mutate(timedate = dmy_hms(paste(Date, Time))) %>%
  dplyr::select(-c("Time", "Date")) %>%
  relocate(timedate) %>%
  as_tibble()

summary(airqual)
head(airqual)
```

Contains the responses of a gas multisensor device deployed on the field in an Italian city. Hourly responses averages are recorded along with gas concentrations references from a certified analyzer.
Multivariate (15) and time series
Has missing values.


### Hints

-   See options of `read.table()` for correct import

## Part B: Import and Visualize

-   Load the data and convert to tsibble.
    -   Make sure dates and hours are converted into proper time objects
    -   Remove incomplete days at beginning and end of data
-   Plot the data as is, preferably as multiple panels in a single plot
-   Describe the data. What is most striking?

```{r}

airqual %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
    geom_line() + 
    theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```
There seems to be multiple -200 in all the numerical (integer and Categorical) data that seems to be outliers or missing values.
Should probably be removed or imputed from the dataset.

## Part C: PCA of data as is

-   Perform PCA on the data as prepared in B
```{r}
pc <- prcomp(airqual[,-1])
summary(pc)
```
-   Create a screeplot and create biplots for 1st and 2nd and for 2nd and 3rd PCs

### Screeplot

```{r}
plot(pc, type = "l")

pc_v <- data.frame(PC = paste0("PC ", ncol(pc$x)),
                   var_explained = pc$sdev^2 / sum(pc$sdev^2)) %>%
          mutate(cum_explained = cumsum(var_explained))

pp <- pc_v[1:3,] %>%
  pivot_longer(!PC, names_to="Quantity", values_to="Explained") %>%
  ggplot(aes(x = PC, y = Explained, color=Quantity, group=Quantity))+
  geom_line() + geom_point() +
  theme_minimal() +
  labs(title = "Variance Explained", x = "Principal Component", 
       y = "Variance Explained")


pp + geom_label(aes(label = round(Explained, 2)))
```
0.91 of variance is explained in the first 3 principal components

### Biplots
```{r}
biplot(pc, scale=0, col=c('blue', 'red'), xlabs=rep('*', nrow(pc$x[, 1:3])))
```

* Can clearly see the -200 outliers
* PRO8.S3.NOx positivly correlated between PC1 and PC2
* NOX.GT. opposite
-   Plot the scores for the PCs
-   Comment on the results. Can you relate some features to your observations in part B?



### Score plot
```{r}
d_pc <- data.frame(Time=airqual[,1], pc$x[,1:3])
head(d_pc)
```

### Hints

-   `ggfortify` provides `autoplot()` for PCA results for ggplot-style biplots
-   To plot the scores, you can use the same code as for plotting the original data

## Part D: Missing values

-   Identify missing values in the time series

The website says that all -200 are NA

-   Investigate to which degree missing values occur at the same time for multiple sensors
```{r}

```


-   Is one or are multiple sensors behaving peculiarly? How would you handle this?

NHMC.GT has almost all missing values after a certain time, as seen in the visualizion, therfore remove to not skew the result away from real data.

-   Discuss options for handling missing values: (a) drop all time points containing any missing value, (b) impute values for missing values. In case of (b) choose a method for imputation. Justify your decisions.

* **(a)** By dropping all NA we only get real data to do anaylizis on, but we miss out on number of datapoints so our overall accuracy becomes lower. In addition it is not possible as we need a regular time axis.
* **(b)** By imputing we can still use rows that are mostly NA free while having the model be skewed as little as possible by these values.
  - Choose to set the NA to the mean value of the overall dataset, therby making the datapoint the most probable if we were to get it randomly. By ussing rolling average we also get the most probable next step from the previous k steps, therfore making it more locally accurate than just using mean.

-   At the end of this step, you should have a version of the data containing only valid values. Plot these data as in Part B.

```{r}
airqual_c <- airqual %>%
  dplyr::select(-c("NMHC.GT."))

airqual_c[airqual_c == -200] = NA

#airqual_c <- imputeTS::na_mean(airqual_c, option = "mean")
# Missing Value Imputation by Weighted Moving Average, Simple moving average
# Plain rolling average
airqual_c <- imputeTS::na_ma(airqual_c, k = 4, weighting = "simple")

airqual_c %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```

### Hints

-   Remember `imputeTS`
-   You can apply its functions to an entire dataframe, will be done column-wise

## Part E: PCA of cleaned data

-   Perform PCA on the data as prepared in D

```{r}
pca <- prcomp(airqual_c[,-1]) #, center = TRUE, scale. = TRUE
```

-   Create a screeplot and biplots for 1st/2nd, 2nd/3rd, 3rd/4th PC

```{r}
# screeplot
pca <- prcomp(airqual_c[,-1])
plot(pca, type = "l")
```
```{r}
# biplots
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=2,y=3, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=3,y=4, loadings=TRUE, loadings.label=TRUE)
```

Biplot red arrow original axis of data

-   Compute total variance explained by 1st, 1st and 2nd, 1st to 3rd, ... PCs

```{r}
#Variance explained
summary(pca)$importance[3,]
```


-   Choose how many PCs to keep and transform data back to original sample space

Keep to 0.99 mark, so PC1 to PC6

```{r}
t <- airqual_c$timedate
x_1 <- pca$x[, 1:6] %*% t(pca$rotation[, 1:6])
x_2 <- t(pca$center + pca$scale * t(x_1))
```

$$ S = XL \Leftrightarrow SL^{-1} = XLL^{-1} \Leftrightarrow X = SL^{-1} = SL^{T}$$
S : Scores
L : Loadings
X : Original matrix


-   Plot the result against the cleaned data, compare and discuss

```{r}
data.frame(timedate = airqual_c$timedate, x_1) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

data.frame(timedate = airqual_c$timedate, x_2) %>%
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
  geom_line() +
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

airqual_c %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```

Looks mostly the same, but deviates slightly as we removed all after PC7.

-   Also plot the scores, zoom in to short time intervals and look at periodicity

### Whole

```{r}
data.frame(timedate = airqual_c$timedate, pca$x[,1:6]) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  timedate, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```
### Zoomed
```{r}
data.frame(timedate = airqual_c$timedate[1:100], pca$x[1:100,1:6]) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  timedate, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```



-   Can you interpret certain PCs?

```{r}
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
data.frame(pca$rotation)

melt(pca$rotation) %>%
  ggplot(aes(Var2, Var1)) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(fill = value, label = round(value, 3)))
```

By looking at the loadings of PC1 one can see that PT08.S5.O3 is the most important feature with respect to explainable variance.
Scores determine witch which weight each PC enter into each row of X





# Task 2: STL and correlation on weather data

## Part A: Data collection for a single station

Based on material from the lectures, write an R function that can obtain a daily average temperature series for a meteorological station from the Norwegian Met Institute's Frost service. The function shall return a tsibble.

```{r}
.client_id <- str_trim(read_file("client_id.txt"))

# Server to collect data from and resource we want from server
server <- "frost.met.no"
resource <- "observations/v0.jsonld"

# Station(s) we want data for. SN17850 is the station ID for Ã…s (Blindern is SN18700)
sources <- 'SN17850'

# Type of data we want, P1D means daily data
elements <- 'mean(air_temperature P1D)'

# Time range we want data for
reference_time <- '1874-01-01/2023-12-31'

# Specify that we want mean temperature calculated from midnight to midnight
timeoffsets <- 'PT0H'


.query_url <- str_glue("https://{.client_id}@{server}/{resource}?sources={sources}&referencetime={reference_time}&elements={elements}&timeoffsets={timeoffsets}")
```

```{r}
# Set this to TRUE to generate a json file
get_data_from_frost = FALSE
weather_file = "weather_data_json.rds.bz2"
if ( get_data_from_frost ) {
  raw_data <- try(fromJSON(URLencode(.query_url), flatten=TRUE))
  
  if (class(raw_data) != 'try-error') {
    print("Data retrieved from frost.met.no!")
    write_rds(raw_data, weather_file, compress="bz2", text=TRUE)  # JSON represents data as text
    print(str_glue("Raw data (JSON) written to '{weather_file}'"))
  } else {
    print("Error: the data retrieval was not successful!")
  }
} else {
  raw_data <- read_rds(weather_file)
  print(str_glue("Raw data (JSON) read from '{weather_file}'"))
}
```
```{r}
df <- unnest(raw_data$data, cols = c(observations))
```

```{r}
head(df)
```

```{r}
df |> dplyr::select(referenceTime, value) |>
      mutate(referenceTime=as.Date(referenceTime)) |>
      rename(Date=referenceTime, Temp=value) |> 
      as_tsibble(index = Date) -> dc
head(dc)
```





## Part B: Data preparation for a single station

-   Identify gaps in the time series.

```{r}
has_gaps(dc)
gaps <- count_gaps(dc)
head(gaps)
```


-   Assume that gaps up to 31 days are acceptable. Find the earliest date in the time series such that all following data have no gaps longer than 31 days. Limit the time series to this.

```{r}
cutoff_date <-  as.character(tail(gaps[gaps$.n >= 31,], n=1)$.to)
cutoff_date
```
identify that all dates after, even if 31 is included, is 1988-06-17

```{r}
dc_cut <- dc %>% tsibble::filter_index(cutoff_date ~ .) 
head(dc_cut)
```



-   Create a regular time series by filling gaps in the tsibble with n/a-s.

```{r}
dc_cut_filled <- tsibble::fill_gaps(dc_cut, .full = TRUE)

head(dc_cut_filled)

```


-   Impute values for the n/a-s. Justify your choice of imputation method.

Choose to impute using simple rolling average, as you get the adventage of getting the most probable next step as well as being locally probable as opposed to using to mean or median for the whole dataset.

```{r}
dc_cut_filled_imp <- imputeTS::na_ma(dc_cut_filled, k = 4, weighting = "simple")
```


-   You should now have a regular time series with only numeric values.

```{r}
tsibble::has_gaps(dc_cut_filled_imp)
is.numeric(dc_cut_filled_imp$Temp)
dc_cut_filled_imp %>%
  ggplot(aes(Date, Temp)) +
  geom_line()

```


-   Remove all data for 29 February so all years have data for exactly 365 days.

```{r}
dc_cut_filled_imp_noLeap <- dc_cut_filled_imp %>% 
  dplyr::filter(!(month(Date) == 2 & day(Date) == 29))
head(dc_cut_filled_imp_noLeap)
```


-   Combine all this code into a function for re-use later. The function should receive the original tsibble from part A as input and return a new tsibble.

```{r}
timeseries_cleaner <- function(table) {
  gaps <- count_gaps(table)
  cutoff_date <-  as.character(tail(gaps[gaps$.n >= 31,], n=1)$.to)
  dc_cut <- dc %>% tsibble::filter_index(cutoff_date ~ .) 
  dc_cut_filled_imp <- imputeTS::na_ma(dc_cut_filled, k = 4, weighting = "simple")
  dc_cut_filled_imp_noLeap <- dc_cut_filled_imp %>% 
    dplyr::filter(!(month(Date) == 2 & day(Date) == 29))
  
  return(dc_cut_filled_imp_noLeap)
}
```

```{r}
identical(timeseries_cleaner(dc), dc_cut_filled_imp_noLeap)
```



### Hints

-   tidyverse provides functions such as has_gaps() and count_gaps()

## Part C: Exploratory analysis for a single station


-   Plot the temperature data as function of time

```{r}
dc %>%
  ggplot(aes(Date, Temp)) +
  geom_line()
```


-   Create density plots of original data and data with imputed values

```{r}
tsibble::fill_gaps(dc, .full = TRUE) %>%
  imputeTS::na_ma(airqual_c, k = 4, weighting = "simple") %>%
  ggplot(aes(Temp)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black") +
  ggtitle("imputed values")

dc %>%
  ggplot(aes(Temp)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black") +
  ggtitle("original data")
```

Temprature seems to be bimodal

-   Turn the temperature data into a timeseries (ts) object

```{r}
ts_dc <- ts(dc) #tsibble::fill_gaps(dc, .full = TRUE)
head(ts_dc)
```


-   Plot the autocorrelation function for lags up to 5.5 years; describe and discuss your observations

```{r}
acf(dc$Temp, lag.max = 365*5.5)
```

correlation between a time series and its own lagged version
Clear pattern in regression error, can be fixed. Breaks the assumption of normally distributed errors.

-   Also plot the ACF only for short lags, up to four weeks

```{r}
acf(dc$Temp, lag.max = 7*4)
```


-   Select some days distributed throughout the year and plot temperature as function of year for, e.g., 1 October, as a scatter plot. This plot can be useful to choose the seasonality window later (see Figs 7 and 8 in Cleveland et al, 1990)

```{r}
dc %>% 
  dplyr::filter((month(Date) == 8 & day(Date) == 1)) %>%
  ggplot(aes(Date, Temp)) +
  geom_point() +
  ylim(0, NA)
```


## Part D: STL analysis

Sesonal Trend Residual
Loess : local regression
Use loess to estimate sesonal and tred components

-   Perform STL on the data. Explore different values for the seasonality and trend windows (remember that we want to look at trends over many years!), the choice between robust STL or not, and possibly the lowpass filter window. Describe your observations. It might be interesting to look at the ACF of the remainder in the STL result.

```{r}
# Trend over years
dc_c_ts <- ts(dc_cut_filled_imp_noLeap$Temp, frequency = 365) # Leap years removed, no need for 365.25
```


```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=35, robust=TRUE)
autoplot(dc_stl)
```
```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=35, robust=FALSE)
autoplot(dc_stl)
```


### Wide sesonality window

```{r}
# STL wide seasonality window
dc_stl <- stl(dc_c_ts, s.window="periodic", t.window=35, robust=TRUE)
autoplot(dc_stl)
```
```{r}
# STL wide seasonality window
dc_stl <- stl(dc_c_ts, s.window="periodic", t.window=35, robust=FALSE)
autoplot(dc_stl)
```

### STL short seasonality window

```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=5, robust=TRUE)
autoplot(dc_stl)
```

```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=5, robust=FALSE)
autoplot(dc_stl)
```




-   Consult the original STL paper by Cleveland et al. (1990) for suggestions on how to choose STL parameters.

$n_{(s)}$ : the smoothing parameter for the seasonal component "must be carefully tailored to each application"(Cleveland et al. p.9, (1990)). In the `stl()` function those parameters are `s.window` and `s.degree`.

Assume yearly periodicity with daily data, therefore `n_(p) = frequency = 365`

""" \\
STL has 6 parameters: \\
$$n_{(p)} = \text{the number of observations} $$ \\
"""

*   s.window : Should be odd and at least 7, according to Cleveland et al. 
*   s.degree : Should be zero or none
*   t.window : span in lags of the loess window for trend extraction, should be odd.

-   Based on your analysis, can you suggest a set of STL parameters to use for further work?

```{r}

```



## Part E: Multiple station analysis

-   Obtain data from eight more stations. Two should be in the same part of Norway as the station from part A; then choose three stations each from two other parts of Norway. Data should cover several decades at least, so look for stations with long series.

```{r}

```


-   Preprocess the data as described in Part B. Find the latest starting date of any series and create a multivariate time series with data from all nine stations starting at this date.

```{r}

```


-   Obtain the cross-correlation matrix between the nine stations. Is there any structure in this 9x9 matrix?

```{r}

```


-   Perform STL individually on each of the nine stations using the parameters from part D. Compare the resulting trends. Are all STL results of equal quality?

```{r}

```


### Hints

You can get a list of all available stations from Frost using

```{r}
#.stations_url = str_glue("https://{.client_id}@frost.met.no/sources/v0.jsonld")
#raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)
```

To limit this to stations with actual data, starting at least as early as 1950, coming from only some parts of Norway relevant columns, and limiting to relevant columns, filter the raw data as

```{r}
# COUNTYS = c(fylke1, fylke2, etc)   # replace with names of "fylker" you are interested in
# 
# stations <- unnest(raw_stations$data, cols='id') |>
#   select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
#   mutate(validFrom=as.Date(validFrom)) |>
#   filter(`@type` == "SensorSystem" & validFrom <= "1950-01-01" & country == "Norge" & county %in% COUNTYS)
```

## Part F (bonus): PCA

-   Perform PCA on the multivariate time series.
