---
title: "Assignment 2"
author: "Group 4"
date: "2024-09-16"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r imports, results='hide', message=FALSE}
options(contrasts = c("contr.sum", "contr.poly"))
require("ggplot2")
require("dplyr")
require("ppcor")
require("caret")
require("tidyr")
require("stringr")
require("lubridate")
require("tsibble")
require("ggfortify")
require("gridExtra")
require("reshape2")

library(imputeTS)   # Time series missing value imputation

library(jsonlite)   # handle JSON data returned by Frost
library(tidyr)      # unpack data from JSON format
library(tidyverse)  # data manipulation with mutate etc, string formatting
library(lubridate)  # process date and time information
library(tsibble)    # special tibbles for time series
library(fpp3)       # autoplot() and gg_season() for time series
library(readr)      # to read the Frost client ID from file
```

# Task 1: Dimension reduction on air quality data

## Part A: Get

-   Obtain data from <https://archive.ics.uci.edu/dataset/360/air+quality>.
-   Provide a brief description of the data based on the information from the website.

```{r}
airquality <- read.table("AirQualityUCI.csv", sep=";", dec=",", header= T)

airqual <- airquality %>%
  dplyr::select(-c("X", "X.1")) %>%
  na.omit() %>%
  mutate(timedate = dmy_hms(paste(Date, Time))) %>%
  dplyr::select(-c("Time", "Date")) %>%
  relocate(timedate) %>%
  as_tibble()

summary(airqual)
head(airqual)
```

Contains the responses of a gas multisensor device deployed on the field in an Italian city. Hourly responses averages are recorded along with gas concentrations references from a certified analyzer.
Multivariate (15) and time series
Has missing values.


### Hints

-   See options of `read.table()` for correct import

## Part B: Import and Visualize

-   Load the data and convert to tsibble.
    -   Make sure dates and hours are converted into proper time objects
    -   Remove incomplete days at beginning and end of data
-   Plot the data as is, preferably as multiple panels in a single plot
-   Describe the data. What is most striking?

```{r}

airqual %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
    geom_line() + 
    theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```
There seems to be multiple -200 in all the numerical (integer and Categorical) data that seems to be outliers or missing values.
Should probably be removed or imputed from the dataset.

## Part C: PCA of data as is

-   Perform PCA on the data as prepared in B
```{r}
pc <- prcomp(airqual[,-1])
summary(pc)
```
-   Create a screeplot and create biplots for 1st and 2nd and for 2nd and 3rd PCs

### Screeplot

```{r}
plot(pc, type = "l")

pc_v <- data.frame(PC = paste0("PC ", ncol(pc$x)),
                   var_explained = pc$sdev^2 / sum(pc$sdev^2)) %>%
          mutate(cum_explained = cumsum(var_explained))

pp <- pc_v[1:3,] %>%
  pivot_longer(!PC, names_to="Quantity", values_to="Explained") %>%
  ggplot(aes(x = PC, y = Explained, color=Quantity, group=Quantity))+
  geom_line() + geom_point() +
  theme_minimal() +
  labs(title = "Variance Explained", x = "Principal Component", 
       y = "Variance Explained")


pp + geom_label(aes(label = round(Explained, 2)))
```
0.91 of variance is explained in the first 3 principal components

### Biplots
```{r}
biplot(pc, scale=0, col=c('blue', 'red'), xlabs=rep('*', nrow(pc$x[, 1:3])))
```

* Can clearly see the -200 outliers
* PRO8.S3.NOx positivly correlated between PC1 and PC2
* NOX.GT. opposite
-   Plot the scores for the PCs
-   Comment on the results. Can you relate some features to your observations in part B?



### Score plot
```{r}
d_pc <- data.frame(Time=airqual[,1], pc$x[,1:3])
head(d_pc)
```

### Hints

-   `ggfortify` provides `autoplot()` for PCA results for ggplot-style biplots
-   To plot the scores, you can use the same code as for plotting the original data

## Part D: Missing values

-   Identify missing values in the time series

The website says that all -200 are NA

-   Investigate to which degree missing values occur at the same time for multiple sensors
```{r}

```


-   Is one or are multiple sensors behaving peculiarly? How would you handle this?

NHMC.GT has almost all missing values after a certain time, as seen in the visualizion, therfore remove to not skew the result away from real data.

-   Discuss options for handling missing values: (a) drop all time points containing any missing value, (b) impute values for missing values. In case of (b) choose a method for imputation. Justify your decisions.

* **(a)** By dropping all NA we only get real data to do anaylizis on, but we miss out on number of datapoints so our overall accuracy becomes lower.
* **(b)** By imputing we can still use rows that are mostly NA free while having the model be skewed as little as possible by these values.
  - Choose to set the NA to the mean value of the overall dataset, therby making the datapoint the most probable if we were to get it randomly.

-   At the end of this step, you should have a version of the data containing only valid values. Plot these data as in Part B.

```{r}
airqual_c <- airqual %>%
  dplyr::select(-c("NMHC.GT."))

airqual_c[airqual_c == -200] = NA

#airqual_c <- imputeTS::na_mean(airqual_c, option = "mean")
# rolling mean
airqual_c <- imputeTS::na_ma(airqual_c, k = 4, weighting = "simple")
airqual_c %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```

### Hints

-   Remember `imputeTS`
-   You can apply its functions to an entire dataframe, will be done column-wise

## Part E: PCA of cleaned data

-   Perform PCA on the data as prepared in D

```{r}
pca <- prcomp(airqual_c[,-1])
```

-   Create a screeplot and biplots for 1st/2nd, 2nd/3rd, 3rd/4th PC

```{r}
# screeplot
pca <- prcomp(airqual_c[,-1])
plot(pca, type = "l")
```
```{r}
# biplots
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=2,y=3, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=3,y=4, loadings=TRUE, loadings.label=TRUE)
```


-   Compute total variance explained by 1st, 1st and 2nd, 1st to 3rd, ... PCs

```{r}
#Variance explained
summary(pca)$importance[3,]
```


-   Choose how many PCs to keep and transform data back to original sample space

Keep to 0.99 mark, so PC1 to PC7

```{r}
t <- datetime
x_1 <- pca$x[, 1:7] %*% t(pca$rotation[, 1:7])
x_2 <- t(pca$center + pca$scale * t(x_1))
```


-   Plot the result against the cleaned data, compare and discuss

```{r}
data.frame(timedate = airqual_c$timedate, x_1) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

# data.frame(timedate = airqual_c$timedate, x_2) %>% 
#   pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
#   ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
#   geom_line() + 
#   theme_minimal() +
#   facet_wrap(~ Features, scales = "free_y", ncol = 3)

airqual_c %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```

Looks mostly the same, but deviates slightly as we removed all after PC7.

-   Also plot the scores, zoom in to short time intervals and look at periodicity

### Whole

```{r}
data.frame(timedate = airqual_c$timedate, pca$x[,1:7]) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  timedate, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```
### Zoomed
```{r}
data.frame(timedate = airqual_c$timedate[1:100], pca$x[1:100,1:7]) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  timedate, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```



-   Can you interpret certain PCs?

```{r}
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
data.frame(pca$rotation)

melt(pca$rotation) %>%
  ggplot(aes(Var2, Var1)) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(fill = value, label = round(value, 3)))
```

By looking at the loadings of PC1 one can see that PT08.S5.O3 is the most important feature with respect to explainable variance.





# Task 2: STL and correlation on weather data

## Part A: Data collection for a single station

Based on material from the lectures, write an R function that can obtain a daily average temperature series for a meteorological station from the Norwegian Met Institute's Frost service. The function shall return a tsibble.

```{r}
.client_id <- str_trim(read_file("client_id.txt"))

# Server to collect data from and resource we want from server
server <- "frost.met.no"
resource <- "observations/v0.jsonld"

# Station(s) we want data for. SN17850 is the station ID for Ã…s (Blindern is SN18700)
sources <- 'SN17850'

# Type of data we want, P1D means daily data
elements <- 'mean(air_temperature P1D)'

# Time range we want data for
reference_time <- '1874-01-01/2023-12-31'

# Specify that we want mean temperature calculated from midnight to midnight
timeoffsets <- 'PT0H'


.query_url <- str_glue("https://{.client_id}@{server}/{resource}?sources={sources}&referencetime={reference_time}&elements={elements}&timeoffsets={timeoffsets}")

#.stations_url = str_glue("https://{.client_id}@frost.met.no/sources/v0.jsonld")
#raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)

# COUNTYS = c(fylke1, fylke2, etc)   # replace with names of "fylker" you are interested in
# 
# stations <- unnest(raw_stations$data, cols='id') |>
#   select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
#   mutate(validFrom=as.Date(validFrom)) |>
#   filter(`@type` == "SensorSystem" & validFrom <= "1950-01-01" & country == "Norge" & county %in% COUNTYS)

```

```{r}
# Set this to TRUE to generate a json file
get_data_from_frost = FALSE
weather_file = "weather_data_json.rds.bz2"
if ( get_data_from_frost ) {
  raw_data <- try(fromJSON(URLencode(.query_url), flatten=TRUE))
  
  if (class(raw_data) != 'try-error') {
    print("Data retrieved from frost.met.no!")
    write_rds(raw_data, weather_file, compress="bz2", text=TRUE)  # JSON represents data as text
    print(str_glue("Raw data (JSON) written to '{weather_file}'"))
  } else {
    print("Error: the data retrieval was not successful!")
  }
} else {
  raw_data <- read_rds(weather_file)
  print(str_glue("Raw data (JSON) read from '{weather_file}'"))
}
```
```{r}
df <- unnest(raw_data$data, cols = c(observations))
```

```{r}
head(df)
```

```{r}
df |> dplyr::select(referenceTime, value) |>
      mutate(referenceTime=as.Date(referenceTime)) |>
      rename(Date=referenceTime, Temp=value) |> 
      mutate(Year=year(Date), Week=week(Date),
             Decade = as.factor(Year - Year %% 10)) |>
      as_tibble() -> dc
dc

```





## Part B: Data preparation for a single station

-   Identify gaps in the time series.
-   Assume that gaps up to 31 days are acceptable. Find the earliest date in the time series such that all following data have no gaps longer than 31 days. Limit the time series to this.
-   Create a regular time series by filling gaps in the tsibble with n/a-s.
-   Impute values for the n/a-s. Justify your choice of imputation method.
-   You should now have a regular time series with only numeric values.
-   Remove all data for 29 February so all years have data for exactly 365 days.
-   Combine all this code into a function for re-use later. The function should receive the original tsibble from part A as input and return a new tsibble.

### Hints

-   tidyverse provides functions such as has_gaps() and count_gaps()

## Part C: Exploratory analysis for a single station

-   Plot the temperature data as function of time
-   Create density plots of original data and data with imputed values
-   Turn the temperature data into a timeseries (ts) object
-   Plot the autocorrelation function for lags up to 5.5 years; describe and discuss your observations
-   Also plot the ACF only for short lags, up to four weeks
-   Select some days distributed throughout the year and plot temperature as function of year for, e.g., 1 October, as a scatter plot. This plot can be useful to choose the seasonality window later (see Figs 7 and 8 in Cleveland et al, 1990)

## Part D: STL analysis

-   Perform STL on the data. Explore different values for the seasonality and trend windows (remember that we want to look at trends over many years!), the choice between robust STL or not, and possibly the lowpass filter window. Describe your observations. It might be interesting to look at the ACF of the remainder in the STL result.
-   Consult the original STL paper by Cleveland et al. (1990) for suggestions on how to choose STL parameters.
-   Based on your analysis, can you suggest a set of STL parameters to use for further work?

## Part E: Multiple station analysis

-   Obtain data from eight more stations. Two should be in the same part of Norway as the station from part A; then choose three stations each from two other parts of Norway. Data should cover several decades at least, so look for stations with long series.
-   Preprocess the data as described in Part B. Find the latest starting date of any series and create a multivariate time series with data from all nine stations starting at this date.
-   Obtain the cross-correlation matrix between the nine stations. Is there any structure in this 9x9 matrix?
-   Perform STL individually on each of the nine stations using the parameters from part D. Compare the resulting trends. Are all STL results of equal quality?

### Hints

You can get a list of all available stations from Frost using

```{r}
#.stations_url = str_glue("https://{.client_id}@frost.met.no/sources/v0.jsonld")
#raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)
```

To limit this to stations with actual data, starting at least as early as 1950, coming from only some parts of Norway relevant columns, and limiting to relevant columns, filter the raw data as

```{r}
# COUNTYS = c(fylke1, fylke2, etc)   # replace with names of "fylker" you are interested in
# 
# stations <- unnest(raw_stations$data, cols='id') |>
#   select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
#   mutate(validFrom=as.Date(validFrom)) |>
#   filter(`@type` == "SensorSystem" & validFrom <= "1950-01-01" & country == "Norge" & county %in% COUNTYS)
```

## Part F (bonus): PCA

-   Perform PCA on the multivariate time series.
