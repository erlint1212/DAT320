---
title: "Assignment 2"
author: "Group 4"
date: "2024-09-16"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r imports, results='hide', message=FALSE}
options(contrasts = c("contr.sum", "contr.poly"))
require("ggplot2")
require("dplyr")
require("ppcor")
require("caret")
require("tidyr")
require("stringr")
require("lubridate")
require("tsibble")
require("ggfortify")
require("gridExtra")
require("reshape2")

library(ggcorrplot) # Easy cross-corrleation plots
library(imputeTS)   # Time series missing value imputation

library(jsonlite)   # handle JSON data returned by Frost
library(tidyr)      # unpack data from JSON format
library(tidyverse)  # data manipulation with mutate etc, string formatting
library(lubridate)  # process date and time information
library(tsibble)    # special tibbles for time series
library(fpp3)       # autoplot() and gg_season() for time series
library(readr)      # to read the Frost client ID from file
```

# Task 1: Dimension reduction on air quality data

## Part A: Get

-   Obtain data from <https://archive.ics.uci.edu/dataset/360/air+quality>.
-   Provide a brief description of the data based on the information from the website.

```{r}
airquality <- read.table("AirQualityUCI.csv", sep=";", dec=",", header= T)

airqual <- airquality %>%
  dplyr::select(-c("X", "X.1")) %>%
  na.omit() %>%
  mutate(timedate = dmy_hms(paste(Date, Time))) %>%
  dplyr::select(-c("Time", "Date")) %>%
  relocate(timedate) %>%
  as_tibble()

summary(airqual) 
head(airqual)
```

Contains the responses of a gas multisensor device deployed on the field in an Italian city. Hourly responses averages are recorded along with gas concentrations references from a certified analyzer.
Multivariate (15) and time series
Has missing values.

The dataset has 15 columns:

1. Date: Measurement date
2. Time: Measurement time
3. CO(GT): Carbon Monoxide concentration in ppm (ground truth)
4. PT08.S1(CO): Sensor 1, Tin Oxide response related to CO
5. NMHC(GT): Non-Methane Hydrocarbons concentration in micrograms/m³
6. C6H6(GT): Benzene concentration in micrograms/m³
7. PT08.S2(NMHC): Sensor 2, response related to Non-Methane Hydrocarbons
8. NOx(GT): Nitrogen Oxides concentration in parts per billion (ground truth)
9. PT08.S3(NOx): Sensor 3, response related to NOx
10. NO2(GT): Nitrogen Dioxide concentration in parts per billion
11. PT08.S4(NO2): Sensor 4, response related to NO2
12. PT08.S5(O3): Sensor 5, response related to O3 (Ozone)
13. T: Temperature in Celsius
14. RH: Relative Humidity (%)
15. AH: Absolute Humidity (g/m³)

### Hints

-   See options of `read.table()` for correct import

## Part B: Import and Visualize

-   Load the data and convert to tsibble.
    -   Make sure dates and hours are converted into proper time objects
    -   Remove incomplete days at beginning and end of data
-   Plot the data as is, preferably as multiple panels in a single plot
-   Describe the data. What is most striking?

```{r}

airqual %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
    geom_line() + 
    theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```
*There seems to be multiple -200 in all the numerical (integer and Categorical) data that seems to be outliers or missing values. Should probably be removed or imputed from the dataset.
* NHMC.GT. has a lot of NA from early on, should probably remove

## Part C: PCA of data as is

-   Perform PCA on the data as prepared in B
```{r}
pc <- prcomp(airqual[,-1])
summary(pc)
```
-   Create a screeplot and create biplots for 1st and 2nd and for 2nd and 3rd PCs

### Screeplot

```{r}
plot(pc, type = "l")

pc_v <- data.frame(PC = paste0("PC ", ncol(pc$x)),
                   var_explained = pc$sdev^2 / sum(pc$sdev^2)) %>%
          mutate(cum_explained = cumsum(var_explained))

pp <- pc_v[1:3,] %>%
  pivot_longer(!PC, names_to="Quantity", values_to="Explained") %>%
  ggplot(aes(x = PC, y = Explained, color=Quantity, group=Quantity))+
  geom_line() + geom_point() +
  theme_minimal() +
  labs(title = "Variance Explained", x = "Principal Component", 
       y = "Variance Explained")


pp + geom_label(aes(label = round(Explained, 2)))
```
0.91 of variance is explained in the first 3 principal components

### Biplots
```{r}
autoplot(pc, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
autoplot(pc, x=2,y=3, loadings=TRUE, loadings.label=TRUE)
```

* Can clearly see the -200 outliers
* PRO8.S3.NOx takes a lot from PC2 than PC1
* NOX.GT. opposite

-   Plot the scores for the PCs
-   Comment on the results. Can you relate some features to your observations in part B?



### Score plot
```{r}
pairs(pc$x[,1:3], main = "PCA Score Plots", pch = 19)
```

### Hints

-   `ggfortify` provides `autoplot()` for PCA results for ggplot-style biplots
-   To plot the scores, you can use the same code as for plotting the original data

## Part D: Missing values

-   Identify missing values in the time series

The website says that all -200 are NA

-   Investigate to which degree missing values occur at the same time for multiple sensors
```{r}
airqual_NA <- airqual
airqual_NA[airqual_NA == -200] = NA
missing_values <- colSums(is.na(airqual_NA)) %>% data.frame()
perc_NA_airqal <- (missing_values/nrow(airqual_NA))*100
```


```{r}
missing_values
```

```{r}
perc_NA_airqal 
```



```{r}
airqual_NA <- airqual
airqual_NA[airqual_NA == -200] = NA

missing_values <- colSums(is.na(airqual_NA))
print("Missing Values in Each Sensor:")
print(missing_values)
# 2. Investigate the degree of missing values

# Visualizing the number of missing values over time

# Create a data frame to visualize missing values
missing_values <- airqual_NA %>%
  mutate(Missing = rowSums(is.na(airqual_NA))) %>%
  dplyr::select(timedate, Missing)

ggplot(missing_values, aes(x = timedate, y = Missing)) +
  geom_line(color = "red") +
  labs(title = "Number of Missing Values Over Time", x = "DateTime", y = "Missing Values Count") +
  theme_minimal()

```

-   Is one or are multiple sensors behaving peculiarly? How would you handle this?

* NHMC.GT has almost all missing values after a certain time, as seen in the visualization, therefore remove to not skew the result away from real data.
* For the rest use imputations as there seems to be no big gaps with NA

-   Discuss options for handling missing values: (a) drop all time points containing any missing value, (b) impute values for missing values. In case of (b) choose a method for imputation. Justify your decisions.

* **(a)** By dropping all NA we only get real data to do anaylizis on, but we miss out on number of datapoints so our overall accuracy becomes lower. In addition it is not possible as we need a regular time axis.
* **(b)** By imputing we can still use rows that are mostly NA free while having the model be skewed as little as possible by these values.
  - Choose to set the NA to the mean value of the overall dataset, therby making the datapoint the most probable if we were to get it randomly. By ussing rolling average we also get the most probable next step from the previous k steps, therfore making it more locally accurate than just using mean.

-   At the end of this step, you should have a version of the data containing only valid values. Plot these data as in Part B.

```{r}
airqual_c <- airqual %>%
  dplyr::select(-c("NMHC.GT."))

airqual_c[airqual_c == -200] = NA

airqual_c <- imputeTS::na_ma(airqual_c, k = 4, weighting = "simple")

airqual_c %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```

Moving average: assumes that the current value ($y_{t}$) is dependent on the error terms including the current error, non linear regression model, order ACF
Auto regressive: assumes that the current value ($y_{t}$) is dependent on previous values, linear regression model, order PACF


## Part E: PCA of cleaned data

-   Perform PCA on the data as prepared in D

```{r}
pca <- prcomp(airqual_c[,-1]) #, center = TRUE, scale. = TRUE
```

-   Create a screeplot and biplots for 1st/2nd, 2nd/3rd, 3rd/4th PC

```{r}
# screeplot
pca <- prcomp(airqual_c[,-1])
plot(pca, type = "l")
```
```{r}
# biplots
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=2,y=3, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=3,y=4, loadings=TRUE, loadings.label=TRUE)
```

Biplot red arrow original axis of data

-   Compute total variance explained by 1st, 1st and 2nd, 1st to 3rd, ... PCs

```{r}
#Variance explained
summary(pca)$importance[3,]
```


-   Choose how many PCs to keep and transform data back to original sample space

Keep to 0.99 mark, so PC1 to PC6

```{r}
t <- airqual_c$timedate
x_1 <- pca$x[, 1:6] %*% t(pca$rotation[, 1:6])
x_2 <- t(pca$center + pca$scale * t(x_1))
```

$$ S = XL \Leftrightarrow SL^{-1} = XLL^{-1} \Leftrightarrow X = SL^{-1} = SL^{T}$$
S : Scores
L : Loadings
X : Original matrix


-   Plot the result against the cleaned data, compare and discuss

```{r}
data.frame(timedate = airqual_c$timedate, x_1) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3) +
  ggtitle("PCA without accounting for scaling")

data.frame(timedate = airqual_c$timedate, x_2) %>%
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
  geom_line() +
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3) +
  ggtitle("PCA accounting for scaling")

airqual_c %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  timedate, y = Measurement, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3) +
  ggtitle("Original data")
```

Looks mostly the same, but deviates slightly as we removed all after PC7.

-   Also plot the scores, zoom in to short time intervals and look at periodicity

### Whole

```{r}
data.frame(timedate = airqual_c$timedate, pca$x[,1:6]) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  timedate, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```
### Zoomed
```{r}
data.frame(timedate = airqual_c$timedate[1:100], pca$x[1:100,1:6]) %>% 
  pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  timedate, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```



-   Can you interpret certain PCs?

```{r}
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
data.frame(pca$rotation)

melt(pca$rotation) %>%
  ggplot(aes(Var2, Var1)) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(fill = value, label = round(value, 3)))
```

By looking at the loadings of PC1 one can see that PT08.S5.O3 is the most important feature with respect to explainable variance.
Scores determine witch which weight each PC enter into each row of X





# Task 2: STL and correlation on weather data

## Part A: Data collection for a single station

Based on material from the lectures, write an R function that can obtain a daily average temperature series for a meteorological station from the Norwegian Met Institute's Frost service. The function shall return a tsibble.

```{r}
.client_id <- str_trim(read_file("client_id.txt"))

# Server to collect data from and resource we want from server
server <- "frost.met.no"
resource <- "observations/v0.jsonld"

# Station(s) we want data for. SN17850 is the station ID for Ås (Blindern is SN18700)
sources <- 'SN17850'

# Type of data we want, P1D means daily data
elements <- 'mean(air_temperature P1D)'

# Time range we want data for
reference_time <- '1874-01-01/2023-12-31'

# Specify that we want mean temperature calculated from midnight to midnight
timeoffsets <- 'PT0H'


.query_url <- str_glue("https://{.client_id}@{server}/{resource}?sources={sources}&referencetime={reference_time}&elements={elements}&timeoffsets={timeoffsets}")
```

```{r}
# Set this to TRUE to generate a json file
get_data_from_frost = TRUE
weather_file = "weather_data_json.rds.bz2"
if ( get_data_from_frost ) {
  raw_data <- try(fromJSON(URLencode(.query_url), flatten=TRUE))
  
  if (class(raw_data) != 'try-error') {
    print("Data retrieved from frost.met.no!")
    write_rds(raw_data, weather_file, compress="bz2", text=TRUE)  # JSON represents data as text
    print(str_glue("Raw data (JSON) written to '{weather_file}'"))
  } else {
    print("Error: the data retrieval was not successful!")
  }
} else {
  raw_data <- read_rds(weather_file)
  print(str_glue("Raw data (JSON) read from '{weather_file}'"))
}
```
```{r}
df <- unnest(raw_data$data, cols = c(observations))
```

```{r}
head(df)
```

```{r}
df |> dplyr::select(referenceTime, value) |>
      mutate(referenceTime=as.Date(referenceTime)) |>
      rename(Date=referenceTime, Temp=value) |> 
      as_tsibble(index = Date) -> dc
head(dc)
```





## Part B: Data preparation for a single station

-   Identify gaps in the time series.

```{r}
has_gaps(dc)
gaps <- count_gaps(dc)
head(gaps)
```


-   Assume that gaps up to 31 days are acceptable. Find the earliest date in the time series such that all following data have no gaps longer than 31 days. Limit the time series to this.

```{r}
# cutoff_date <-  as.character(tail(gaps[gaps$.n >= 31,], n=1)$.to)
# cutoff_date
if (nrow(gaps) != 0) {
  cutoff_date <-  as.character(tail(gaps[gaps$.n >= 31,], n=1)$.to)
  dc_cut <- dc %>% tsibble::filter_index(cutoff_date ~ .) 
} else {
  dc_cut <- dc
}
```
identify that all dates after, even if 31 is included, is 1988-06-17

```{r}
# dc_cut <- dc %>% tsibble::filter_index(cutoff_date ~ .) 
head(dc_cut)
```



-   Create a regular time series by filling gaps in the tsibble with n/a-s.

```{r}
dc_cut_filled <- tsibble::fill_gaps(dc_cut, .full = TRUE)

head(dc_cut_filled)

```


-   Impute values for the n/a-s. Justify your choice of imputation method.

Choose to impute using simple rolling average, as you get the adventage of getting the most probable next step as well as being locally probable as opposed to using to mean or median for the whole dataset.

```{r}
dc_cut_filled_imp <- imputeTS::na_ma(dc_cut_filled, k = 4, weighting = "simple")
```


-   You should now have a regular time series with only numeric values.

```{r}
tsibble::has_gaps(dc_cut_filled_imp)
is.numeric(dc_cut_filled_imp$Temp)
dc_cut_filled_imp %>%
  ggplot(aes(Date, Temp)) +
  geom_line()

```


-   Remove all data for 29 February so all years have data for exactly 365 days.

```{r}
dc_cut_filled_imp_noLeap <- dc_cut_filled_imp %>% 
  dplyr::filter(!(month(Date) == 2 & day(Date) == 29))
head(dc_cut_filled_imp_noLeap)
```


-   Combine all this code into a function for re-use later. The function should receive the original tsibble from part A as input and return a new tsibble.

```{r}
timeseries_cleaner <- function(table) {
  gaps <- count_gaps(table)
  gaps_31 <- gaps[gaps$.n >= 31,]
  if (nrow(gaps_31) != 0) {
    cutoff_date <-  as.character(tail(gaps_31, n=1)$.to)
    table_cut <- table %>% tsibble::filter_index(cutoff_date ~ .)
  } else {
    table_cut <- table
  }
  table_cut_filled <- tsibble::fill_gaps(table_cut, .full = TRUE)
  table_cut_filled_imp <- imputeTS::na_ma(table_cut_filled, k = 4, weighting = "simple")
  table_cut_filled_imp_noLeap <- table_cut_filled_imp %>%
    dplyr::filter(!(month(Date) == 2 & day(Date) == 29))

  return(table_cut_filled_imp_noLeap)
}
```

```{r}
identical(timeseries_cleaner(dc), dc_cut_filled_imp_noLeap)
```



### Hints

-   tidyverse provides functions such as has_gaps() and count_gaps()

## Part C: Exploratory analysis for a single station


-   Plot the temperature data as function of time

```{r}
dc %>%
  ggplot(aes(Date, Temp)) +
  geom_line()
```


-   Create density plots of original data and data with imputed values

```{r}
tsibble::fill_gaps(dc, .full = TRUE) %>%
  imputeTS::na_ma(airqual_c, k = 4, weighting = "simple") %>%
  ggplot(aes(Temp)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black") +
  ggtitle("imputed values")

dc %>%
  ggplot(aes(Temp)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black") +
  ggtitle("original data")
```

Temperature seems to be bimodal

-   Turn the temperature data into a timeseries (ts) object

```{r}
ts_dc <- ts(dc_cut_filled_imp_noLeap, frequency = 365) 
head(ts_dc)
```


-   Plot the autocorrelation function for lags up to 5.5 years; describe and discuss your observations

```{r}
acf(dc$Temp, lag.max = 365*5.5)
```

* How correlated each Tempratrure is to the temprature 5.5 years ago
* Can see clear seasonality in the plot with a time interval of about a year (365 days)
* From eyeballing it seems that there is a positive correlation from jan to jul that decreases over time and a negative correlation that increases over time fron jul to dec
* The correlation seems to be decreasing slightly over time, with the first months being perfectly near perfectly correlated and the last ones being ca 0.7

The autocorrelation analysis helps in detecting hidden patterns and seasonality and in checking for randomness

correlation between a time series and its own lagged version
Clear pattern in regression error, can be fixed. Breaks the assumption of normally distributed errors.

Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals.

Autocorrelation measures the relationship between a variable's current value and its past values.

An autocorrelation of +1 represents a perfect positive correlation, while an autocorrelation of -1 represents a perfect negative correlation.

-   Also plot the ACF only for short lags, up to four weeks

```{r}
acf(dc$Temp, lag.max = 7*4)
```
Hard to see pattern, decreasing correlation over time? (As 1 is perfect positive correlation)


-   Select some days distributed throughout the year and plot temperature as function of year for, e.g., 1 October, as a scatter plot. This plot can be useful to choose the seasonality window later (see Figs 7 and 8 in Cleveland et al, 1990)

```{r}
months <- c("Janaury", "Febrary")
month_plot <- function(data, date) {
  out_plot  <- data %>% 
    dplyr::filter((month(Date) == date & day(Date) == 1)) %>%
    ggplot(aes(Date, Temp)) +
    geom_point() +
    ylim(-25, 25) +
    ggtitle(month.name[i])
  
  return(out_plot)
}


month_plots_list <- list()

for (i in 1:12) {
  month_plots_list[[i]] <- month_plot(dc_cut_filled_imp_noLeap, i)
}
  
gridExtra::grid.arrange(grobs = month_plots_list[1:6], ncol = 3)
gridExtra::grid.arrange(grobs = month_plots_list[7:12], ncol = 3)
```

In (Cleveland et al, fig .7 and .8 , 1990) it is shown that by plotting data points for specific days in a month every year you can visually inspect how good of a fit the loess model is to the different months, very useful for determining `s.window`

## Part D: STL analysis

Seasonal Trend Residual, has inner and outer loop from loess using smoothing
inner loop: updates trend and seasonality, use hyperparams to sepperate trend and seasonality, trend superposition seasonality increase in curve
outer loop: compute robustness weights, assessing the residuals.
Loess : local regression, idea: for every data point, fit a polynomial to make a linear fit, quadratic fit
Iteratively improve fit assigning reduced weight to outliers.
Use loess to estimate seasonal and trend components, and residuals

-   Perform STL on the data. Explore different values for the seasonality and trend windows (remember that we want to look at trends over many years!), the choice between robust STL or not, and possibly the low pass filter window. Describe your observations. It might be interesting to look at the ACF of the remainder in the STL result.

If the autocorrelation are small and close to zero, it indicates that the remainders are uncorrelated (resemble white noice)

```{r}
# Trend over years
dc_c_ts <- ts(dc_cut_filled_imp_noLeap$Temp, frequency = 365) #365 Leap years removed, no need for 365.25
```


```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=35, robust=TRUE)
autoplot(dc_stl)
```
Can clearly see the seasonal component in the data and its own plot, but the trend line seems more uncertain. Remainders seems to spike on every seasonal top and bottom.

```{r}
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
```

From the autocorrelation it seems that there is a strong correlation on lower lags but smaller on larger.


```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=100, robust=FALSE)
autoplot(dc_stl)
```
```{r}
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
```

Having robust off seems to make greater remainders than without.


### Wide sesonality window

```{r}
# STL wide seasonality window
dc_stl <- stl(dc_c_ts, s.window="periodic", t.window=35, robust=TRUE)
autoplot(dc_stl)
```
```{r}
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
```
```{r}
# STL wide seasonality window
dc_stl <- stl(dc_c_ts, s.window="periodic", t.window=35, robust=FALSE)
autoplot(dc_stl)
```
```{r}
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
```



### STL short seasonality window

```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=5, robust=TRUE)
autoplot(dc_stl)
```
```{r}
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
```

```{r}
dc_stl <- stl(dc_c_ts, s.window=7, t.window=5, robust=FALSE)
autoplot(dc_stl)
```
```{r}
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
```

Shows very few siginficant spikes in autocorrelation, besides 1




-   Consult the original STL paper by Cleveland et al. (1990) for suggestions on how to choose STL parameters.

`s.window = "periodic"` means take all the data into account and wrap around , will make the seasonal component perioduc, aka amplitude will not scale over time. Do not use if the seasonality changes significantly over time.

If you assume that the seasonal patter is constant through time, you should set `s.window` to a be a big odd number, so that you use your entire data to perform your analysis.
If seasonal patterns evolve quickly, you should use a small number but bigger than 7 and odd, therby only using the most recent data such that the analysis is not affected by old seasonal pattern that are not relevant anymore.

$n_{(s)}$ : the smoothing parameter for the seasonal component "must be carefully tailored to each application"(Cleveland et al. p.9, (1990)). In the `stl()` function those parameters are `s.window` and `s.degree`.

Assume yearly periodicity with daily data, therefore `n_(p) = frequency = 365`

`s.window` is the seasonal smoothing parameter

*   "We will always take $n_{(s)}$ to be odd."(Cleveland et al. p.15, (1990))
*   "we also want $n_{(s)}$ to be at least 7."(Cleveland et al. p.15, (1990))
*   "The choice of appropriate variation depend critically on the characteristics of the series."

diagnostic method:

*   seasonal-diagnostic plot : "helps us decide how much of the variation in the data other than trend should go into the seasonal component and how much in the remainder"(Cleveland et al. p.15, (1990))

"Without robustness, the seasonal component is distorted. This is illustrated by seasonal-diagnostic display"(Cleveland et al. p.15, (1990))


*   s.window : Should be odd and at least 7, according to Cleveland et al. 
*   s.degree : Should be zero or none
*   t.window : span in lags of the loess window for trend extraction, should be odd.
*   

-   Based on your analysis, can you suggest a set of STL parameters to use for further work?

Since the data seems to be seasonally consistent over the years, I would recommend using a big odd number for `s.window` or just `periodic`

### Trend smoothing parameter $n_{(t)}$

## Part E: Multiple station analysis

-   Obtain data from eight more stations. Two should be in the same part of Norway as the station from part A; then choose three stations each from two other parts of Norway. Data should cover several decades at least, so look for stations with long series.


```{r}
.stations_url = str_glue("https://{.client_id}@frost.met.no/sources/v0.jsonld")
raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)
df_stations <- data.frame(raw_stations)
```
```{r}
# 2 from Ås SN17850

unique(df_stations[,"data.county"])
```
```{r}
COUNTYS = c("AKERSHUS", "FINNMARK", "TROMS")   # replace with names of "fylker" you are interested in

stations <- unnest(raw_stations$data, cols='id') |>
  dplyr::select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
  mutate(validFrom=as.Date(validFrom)) |>
  filter(`@type` == "SensorSystem" & validFrom <= "1950-01-01" & country == "Norge" & county %in% COUNTYS)
```

```{r}
cut_stations <- rbind(
  stations[stations$county == "AKERSHUS",][1:2,],
  stations[stations$county == "TROMS",][c(2,3,5),],
  stations[stations$county == "FINNMARK",][c(1,2,4),]
  )
cut_stations
```




-   Preprocess the data as described in Part B. Find the latest starting date of any series and create a multivariate time series with data from all nine stations starting at this date.


```{r}
get_dc_from_frost <- function(src_id, cutoff_date_glob = NA) {
  # Set this to TRUE to generate a json file
  weather_file = paste(src_id, "_weather_data_json.rds.bz2", sep="")
  get_data_from_frost = !file.exists(file = weather_file)
  
  .query_url <- str_glue("https://{.client_id}@{server}/{resource}?sources={src_id}&referencetime={reference_time}&elements={elements}&timeoffsets={timeoffsets}")
  raw_data <- list()
  
  if ( get_data_from_frost ) {
    raw_data <- try(fromJSON(URLencode(.query_url), flatten=TRUE))
    
    if (class(raw_data) != 'try-error') {
      print("Data retrieved from frost.met.no!")
      write_rds(raw_data, weather_file, compress="bz2", text=TRUE)  # JSON represents data as text
      print(str_glue("Raw data (JSON) written to '{weather_file}'"))
    } else {
      print("Error: the data retrieval was not successful!")
      stop()
    }
  } else {
    raw_data <- read_rds(weather_file)
    print(str_glue("Raw data (JSON) read from '{weather_file}'"))
  }
  
  df <- unnest(raw_data$data, cols = c(observations))
  
  df |> dplyr::select(referenceTime, value) |>
        mutate(referenceTime=as.Date(referenceTime)) |>
        rename(Date=referenceTime, Temp=value) |> 
        as_tsibble(index = Date) -> dc
  
  if (!is.na(cutoff_date_glob)) {
    dc <- dc %>% tsibble::filter_index(cutoff_date_glob ~ .) 
  }
  
  return(dc)
}
```



```{r}
station_list <- c()
for (i in 1:8) {
  station_list <- c(station_list, as.character(timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[i])))$Date[1]))
}
```


```{r}
station_list
start_date_dc <- max(station_list)
start_date_dc
```
```{r}
dc_stations <- timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[1]), start_date_dc)) %>%
  rename(!!paste("Temp", cut_stations$name[1], sep=".") := "Temp")

for (i in 2:8) {
  dc_stations[paste("Temp", cut_stations$name[i], sep=".")] <- timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[i]), start_date_dc))$Temp
}
```
```{r}
head(dc_stations)
```


-   Obtain the cross-correlation matrix between the nine stations. Is there any structure in this 9x9 matrix?

```{r}
dc_stations %>%
  subset(select = -c(Date)) %>%
  cor() %>%
  round(2) %>%
  data.frame() %>%
  ggcorrplot(type = "upper", lab = TRUE)
```

* The temperatures that comes from the same fylke seems to be more correlated, and the closer the fylke is to each other the more correlated they are. 
* Everything positively correlated

-   Perform STL individually on each of the nine stations using the parameters from part D. Compare the resulting trends. Are all STL results of equal quality?

```{r}
p_list <- list() 
for (colname in colnames(dc_stations)) {
  dc_stl <- stl(ts(dc_stations[[colname]], frequency = 365), s.window=41, t.window=11, robust=TRUE)
  pl <- autoplot(dc_stl) +
    ggtitle(colname)
  p_list[[colname]] <- pl
}

```

As the trend seems to be relatively flat in all examples, I can't see any significant difference in quality of the trend lines. Remainders seems to be between 10 and -10 in all the plots so the quality seems to be about the same in all plots. Except Kirekenes Lufthavn and Kautekeino where the remainder range seems to be between [10, -20], a significant deviant from the other plots.


```{r}
p_list[[2]]
```
```{r}
p_list[[3]]
```
```{r}
p_list[[4]]
```
```{r}
p_list[[5]]
```
```{r}
p_list[[6]]
```
```{r}
p_list[[7]]
```
```{r}
p_list[[8]]
```
```{r}
p_list[[9]]
```


## Part F (bonus): PCA

-   Perform PCA on the multivariate time series.

```{r}
mts_pca <- dc_stations %>%
  subset(select = -c(Date)) %>%
  prcomp(center = TRUE)
```

```{r}
plot(mts_pca, type = "l")
```

Can clearly see that most the variance is captured in the first two PC


```{r}
data.frame(date = dc_stations$Date, mts_pca$x) %>% 
  pivot_longer(!date, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  date, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)
```

```{r}
data.frame(date = dc_stations$Date[1:600], mts_pca$x[1:600,]) %>% 
  pivot_longer(!date, names_to="Features", values_to="Scores") %>%
  ggplot(aes(x =  date, y = Scores, col = Features)) + 
  geom_line() + 
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3)

```

```{r}
t <- dc_stations$Date
mts_x_1 <- mts_pca$x[, 1:2] %*% t(mts_pca$rotation[, 1:2])
mts_x_2 <- t(mts_pca$center + mts_pca$scale * t(mts_x_1))
```

```{r}
data.frame(Date = dc_stations$Date, mts_x_1) %>%
  pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  Date, y = Measurement, col = Features)) +
  geom_line() +
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3) +
  ggtitle("mts_pca without accounting for scaling")

data.frame(Date = dc_stations$Date, mts_x_2) %>%
  pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  Date, y = Measurement, col = Features)) +
  geom_line() +
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3) +
  ggtitle("mts_pca accounting for scaling")

dc_stations %>%
  pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
  ggplot(aes(x =  Date, y = Measurement, col = Features)) +
  geom_line() +
  theme_minimal() +
  facet_wrap(~ Features, scales = "free_y", ncol = 3) +
  ggtitle("Original data")
```

