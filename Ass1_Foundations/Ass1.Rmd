---
title: "Assignment 1"
author: 
- Erling Tenn√∏y Nordtvedt 
- Oleg Karpov
date: "2024-09-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(contrasts = c("contr.sum", "contr.poly"))
require("ggplot2")
require("dplyr")
```

# Exercise 1 - R syntax & data structures

## (a)
```{r E1_a}
gapminder <- read.csv("gapminder.csv")

summary(gapminder)
```

## (b)
``` {r E1_b}
gapminder %>%
  ggplot(aes(x=year, y=lifeExp, colour=continent)) +
    #geom_bar(position='dodge', stat='identity')
    geom_line()
```

## (c)
```{r E1_c}
knitr::kable(
  gapminder %>%
    group_by(continent, year) %>%
    summarise_at(vars(lifeExp), list(Min = min, Med = median, Mean = mean, Max = max, Sd = sd)) %>%
    data.frame()
)
```

## (d)
```{r E1_d}
gapminder %>%
  group_by(continent, year) %>%
  summarise(Mean_lifeExp= mean(lifeExp, na.rm = T), .groups = 'drop') %>%
  ggplot(aes(x=year, y=Mean_lifeExp)) +
  geom_ribbon(aes(ymin= Mean_lifeExp - 1, ymax = Mean_lifeExp + 1), fill = "grey70") +
  geom_line() +
  facet_grid(.~continent) +
  theme(axis.text.x = element_text(angle=90)) +
  ylim(0, NA)
```

# Exercise 2 - Elementary data analysis and model training

## (a)
```{r E2_a_1}
weatherHistory <- read.csv("weatherHistory.csv")
head(weatherHistory)
```
**Qualitative nominal**

* Summary
* Precip.Type
* Daily.Summary

**Quantitative Continuous:**

* Temperature..C.
* Apparent.Temperature..C.
* Humidity
* Wind.Speed..km.h.
* Visibility..km.
* Wind.Bearing..degrees (Reason: Not ranked)

**Quantitative Discrete:**

* Formatted.Date
* Loud.Cover

### Qualitative nominal
``` {r E2_a_2}
weatherHistory %>%
  group_by(Precip.Type) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Precip.Type, y=Count)) +
  geom_bar(stat='identity', position='dodge')
```


``` {r E2_a_3}
weatherHistory %>%
  group_by(Summary) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Summary, y=Count)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle=90))
```
``` {r E2_a_4}
weatherHistory %>%
  group_by(Daily.Summary) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Daily.Summary, y=Count)) +
  geom_bar(stat='identity', position='dodge')
```


### Discrete nominal
```{r E2_a_5}
weatherHistory %>%
  ggplot(aes(Temperature..C.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```


```{r E2_a_6}
weatherHistory %>%
  ggplot(aes(Apparent.Temperature..C.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```


```{r E2_a_7}
weatherHistory %>%
  ggplot(aes(Humidity)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```
```{r E2_a_8}
weatherHistory %>%
  ggplot(aes(Wind.Speed..km.h.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```
```{r E2_a_9}
weatherHistory %>%
  ggplot(aes(Visibility..km.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```

``` {r E2_a_11}

# weatherHistory %>%
#   group_by(Wind.Bearing..degrees.) %>%
#   summarize(Count = n()) %>%
#   ggplot(aes(x=Wind.Bearing..degrees., y=Count)) +
#   geom_bar(stat='identity', position='dodge') +
#   theme(axis.text.x = element_text(angle=90))

weatherHistory %>%
  ggplot(aes(Wind.Bearing..degrees.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```

### Quantative discrete
```{r E2_a_10}
weatherHistory %>%
  group_by(Loud.Cover) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Loud.Cover, y=Count)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle=90))
```

## (b)
First removing all columns that seem irrelevant, reasoning:

* Formatted.Date : When encoded it will be equal to row label (1, 2, 3, ...) which tells nothing
* Loud.Cover : All values are 0, therfore tells nothing
* Daily.Summary : Too big to onehotencode effectivly

Then remove all rows with NA, do this after removing irrelevant columns so data is not lost to having NA in the removed columns
```{r E2_b_1}
library(caret)
library(tidyr)
library(dplyr)
weatherHistory <- weatherHistory %>% select(-c("Formatted.Date", "Daily.Summary", "Loud.Cover"))
weatherHistory <- na.omit(weatherHistory) # Remove all NA
head(weatherHistory)
```

``` {r E2_b_2}

num_wH <- weatherHistory %>%
  select(-c("Summary", "Precip.Type")) 
num_stand_wH <- as.data.frame(sapply(num_wH, function(x) ((x-mean(x))/sd(x))))

qualitative_wH <- weatherHistory %>%
  select(c("Summary", "Precip.Type")) #Omitted "Formatted.Date","Daily.Summary"

# PT <- factor(qualitative_wH$Precip.Type)
# PT <- as.data.frame(model.matrix(~ Precip.Type - 1, PT))
# 
# FD <- factor(qualitative_wH$Summary)
# FD <- as.data.frame(model.matrix(~ f - Summary - 1, FD))

q1 <- table(1:nrow(weatherHistory), weatherHistory$Precip.Type) # as.data.frame.matrix(
q2 <- table(1:nrow(weatherHistory), weatherHistory$Summary)
q <- as.data.frame.matrix(cbind(q1, q2))
#head(merge(PT, FD))

#oh_weatherHistory <- dummyVars("~ .", data = qualitative_wH)
#oh_weatherHistory <- data.frame(predict(oh_weatherHistory, newdata = qualitative_wH))
#head(num_stand_wH)
cleaned_wH <- cbind(num_stand_wH, q)
head(cleaned_wH)

sample <- sample(c(T, F), nrow(cleaned_wH), replace=T, prob=c(0.75, 0.25))
test_wH <- cleaned_wH[!sample,]
train_wH <- cleaned_wH[sample,]
```
## (c)
Reason for choosen variables:

* Tempratrue (C) : Baseline that gets moved
* Humidity : Feels a lot hotter when its more humid, harder to sweat
* Wind speed : Wind makes skin feel colder
* Pressure : Pressure changes based on if it may rain or not, feels different
* Rain/Snow : If it rains the air feels colder

```{r E2_c_1}
wH_lm <- train_wH %>%
  lm(Apparent.Temperature..C. ~ rain + snow + Pressure..millibars. + Humidity + Temperature..C. + Wind.Speed..km.h., .)

summary.aov(wH_lm)
summary(wH_lm)
```
As can be seen an the ANOVA and t test for the different values, they are all significant within $\alpha \approx 0$ which means that there is almost 0 chance that the factors are due to random chance. (FIX LATER, DOUBBLE CHECK)
```{r E2_2_vals}
y_test_true <- test_wH$Apparent.Temperature..C.
y_test_pred <- predict(wH_lm, newdata = test_wH)
y_train_true <- train_wH$Apparent.Temperature..C.
y_train_pred <- predict(wH_lm, newdata = train_wH)
```

### RMSE

$$ \text{RMSE}(y, \hat{y}) = \sqrt{\frac{\sum_{i=0}^{N - 1} (y_i - \hat{y}_i)^2}{N}} $$
```{r E2_c_2}
rmse_test <- RMSE(y_test_pred, y_test_true)
rmse_train <- RMSE(y_train_pred, y_train_true)
rmse_test
rmse_test

```
### MAE
$$ \text{MAE}(x,y) = \sum_{i=1}^{D}|x_i-y_i|  $$
```{r E2_c_3}
mae_test <- MAE(y_test_pred, y_test_true)
mae_train <- MAE(y_train_pred, y_train_true)
mae_test
mae_train
```

### $\text{R}^{2}$ score (coefficient of determination)
$$ 
R^2 = 1 - \frac{\text{SSR (sum of square regression)}}{\text{SST (total sum of squares)}} 
= 1 - \frac{\sum_{i=0}^{N - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{N - 1} (y_i - \hat{y})^2}
$$
```{r E2_c_4}
R2_test <- R2(y_test_pred, y_test_true)
R2_train <- R2(y_train_pred, y_train_true)
R2_test
R2_train
```

# Exercise 3 - Linear Regression and Diagnostic Plots

## (a)

### a) Linearity

If a function is linear, then all factors increase at a consistent rate for each step.
Gradient and all partial first order derivatives will be a function of constants
Linear function: $y(x,z) = 2x + 4z + 5xz + 10$
Non-Linear function: $y(x,z) = 2x^2 + 5z^5$

### b) Homoscedasticity 

homogeneity of variance assumes that all observations are picked from a sources that have equal variance.
In other words the data points around a linear model should vary equally from the line, if there are any cone shape or other such irregularities this assumption is broken and the model that will be produced will eb flawed.

### c) Independence

Observations can't depend on each other, in other words if you pick one observation from a population for your sample, this should not affect the next sample you choose. In other words no observations should depend or affect each other.

By the very nature of lm, it is **assumed** that you have a i.i.d dataset. This has to be done in the sampeling stage, not the cleaning stage.

### d) Normaility

Assumes that the data follows a normal distribution.

Can be tested by making a Q-Q plot and seeing how well the data points follows  the line.
If the right tail is very heavy, you probably should log the value
slightly heavy tails can still be used because of the law of large numbers.


## (b)

These are all diagnostic plots that allow us to test our assumptions

### Residuals vs Fitted
To test for the assumption of linearity
```{r E3_b_1}
plot(wH_lm,1)
```

### Normal Q-Q
To test for normality of residuals
```{r E3_b_2}
plot(wH_lm,2)
```

### Scale-Location (or Spread-Location)
To test for homoscadescity (equality of variance)
```{r E3_b_3}
plot(wH_lm,3)
```

### Residuals vs Leverage
Allows us to identify influential observations
**Leverage:** extent to which the coefficients in the regression model would change if a particular observations was removed from the dataset (outliers).
**Standarized residuals:** standardized difference between a predicted value for an observation and the actual value of the observation.
```{r E3_b_4}
plot(wH_lm,5)
```

## (c)
```{r E3_c_setup}
data_gen <- function(c=0, t=0, cm=0, cp=100, cs=0, nnm=1, nnp=10000, snn=500, nnnm=0, nnnp=10000, snnn=500) {
  set.seed(42)
  
  n <- 1000
  x <- 1:n
  
  # Changeable parameters
  # - Change the parameters to affect the generated data points below.
  # - You may copy this code multiple times to answer all the questions in the exercise.
  # - You may find it reasonable to argue for multiple violations from a single generated set of data points.
  
  contant <- c 
  trend <- t
  curve_magnitue <- cm
  curve_period <- cp
  curve_shift <- cs
  normal_noise_magnitue <- nnm
  norm_noise_periode <- nnp
  shift_norm_noise <- snn
  non_normal_noise_magnitue <- nnnm
  non_norm_noice_periode <- nnnp
  shift_non_norm_noise <- snnn
  
  y.gen <- contant +
    trend * x + 
    curve_magnitue* sin(
      (x/curve_period + curve_shift)*pi
      ) + 
    normal_noise_magnitue*cos(
      (x/norm_noise_periode + shift_norm_noise/norm_noise_periode)*pi
      )*rnorm(n, sd = 3) +
    non_normal_noise_magnitue*cos(
      (x/non_norm_noice_periode + shift_non_norm_noise/non_norm_noice_periode)*pi
      ) * rexp(n, rate = 0.2) 
  
  p <- qplot(x, y.gen, ylab = "y") +
    geom_point(size = 0.1) +
    labs(title = "Data generate for linear regrestion")
  
  # Display the plot
  print(p)
  return(list("x"=x,"y.gen"=y.gen))
}
```

### Holds all the assumptions

```{r E3_c_plots}
gen_data <- data_gen()
lm.gen <- lm(gen_data$y.gen ~ gen_data$x)
plot(lm.gen, which = 1)
plot(lm.gen, which = 2)
plot(lm.gen, which = 3)
plot(lm.gen, which = 5)
```

### Breaks the assumption of Homoscedastiicty

```{r E3_c_bHomoscedastiicty}
gen_data <- data_gen(nnnm = -10000, snnn = 5000, nnnp = 50000)
lm.gen <- lm(gen_data$y.gen ~ gen_data$x)
plot(lm.gen, which = 2)
```

### Breaks the assumption of Linearity

```{r E3_c_bLinear}
gen_data <- data_gen(nnnm = -10000, snnn = 5000, cs=10000, cm = 10000, snn = 10000)
lm.gen <- lm(gen_data$y.gen ~ (gen_data$x)^4)
plot(lm.gen, 1)
```

### Breaks the assumption of Normality.
```{r E3_c_bNorm}
gen_data <- data_gen(nnnm = 10000, snnn = 5000, cs=10000, cm = 10000, snn = 10000, nnm = 0, nnnp = 5000, t = 1)
lm.gen <- lm(gen_data$y.gen ~ gen_data$x)
plot(lm.gen, 2)
```

## (d)

### Breaks Homoscedasticity

The error distibution is not consistent, aka $$ \epsilon_i \ne \sigma^2 $$ instead it is
$$ \sigma_{i}^2 = x_{i} \sigma^2 $$, in other words heteroscedastic.

If plot X against Y you will see a cone shape instead of expected line, clearly there are different distributions of variances based on X

### Breaks Linearity

The Residual vs Fitted shows a clear cone as well as a clear non-linear pattern between X and Y

### Breaks Normality

The Q-Q Normal plot clearly shows high bias towards the tails


# Exercise 4 - correlation and partial correlation

## (a)

**Correlation:** Degree to which a pair of variables/factors are linearly related. As in the increase in one variable either increases or decreases another. Correlation does not imply causation, even if Smoking and higher life expectancy is higher in one area, that doesn't automatically imply that smoking is healthy.

Higher variance decreases correlation, while higher covariance increases it. In other words the more the two variables/factors trend and the less variance from that trend they have the more correlated the two varaibles/factors are.

## (b)

**Partial correlation:** measures the degree of association between two random variables, with the effect of a set of controlling random variables removed. [Wiki](https://en.wikipedia.org/wiki/Partial_correlation)
Helps to understand the relationship between two variables, controlling for the effect of one or more additional variables.

The formula describes the correlation between the residuals $e_X$ and $e_Y$ resulting from the linear regression of X with Z and of Y with Z, respectively.

### Scenarios

1. You suspect there are inter dependencies between sow weight, food eaten and hormones in food
2. You suspect there are inter dependencies between distance ran, speed per km and water drunk.
3. You suspect there are inter dependencies between rain, pressure and latitude coordinates


## (c)

No clue

## (d)

```{r E4_setup}
weatherHistory <- read.csv("weatherHistory.csv")
select_wH <- weatherHistory %>% select(Temperature..C., Apparent.Temperature..C., Humidity)
```

```{r E4_pairwise}
pairwise_corr <- select_wH %>%
  cor() %>%
  data.frame()
pairwise_corr
rownames(pairwise_corr)
pairwise_corr["Temperature..C.", "Temperature..C."]
```

```{r E4_partialCorr}
#partial_corr <- funcion(X, Y, Z, corrMat) {
#  p_XY <- corrMat[X, Y]
#}
  
```