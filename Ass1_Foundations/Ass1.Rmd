---
title: "DAT320: Compulsory assignment 1"
author: "Group 4"
date: "2024-09-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r imports, results='hide', message=FALSE}
options(contrasts = c("contr.sum", "contr.poly"))
require("ggplot2")
require("dplyr")
require("ppcor")
require("caret")
require("tidyr")
```

# Exercise 1 - R syntax & data structures

## Task a
```{r E1_a}
gapminder <- read.csv("gapminder.csv")

summary(gapminder)
```

## Task b
``` {r E1_b}
gapminder %>%
  group_by(country) %>%
  ggplot(aes(x=year, y=lifeExp, gorup=country,  colour=continent)) +
    geom_line()
```

## Task c
```{r E1_c}
knitr::kable(
  gapminder %>%
    group_by(continent, year) %>%
    summarise_at(vars(lifeExp), list(Min = min, Med = median, Mean = mean, Max = max, Sd = sd)) %>%
    data.frame()
)
```

## Task d
```{r E1_d}
gapminder %>%
  group_by(continent, year) %>%
  summarise(Mean_lifeExp=mean(lifeExp, na.rm = T), SD_lifeExp=sd(lifeExp, na.rm = T), .groups = "drop") %>%
  ggplot(aes(x=year, y=Mean_lifeExp)) +
  geom_ribbon(aes(ymin= Mean_lifeExp - SD_lifeExp, ymax = Mean_lifeExp + SD_lifeExp), fill = "grey70") +
  geom_line() +
  facet_grid(.~continent) +
  theme(axis.text.x = element_text(angle=90)) +
  ylim(0, NA)
```

# Exercise 2 - Elementary data analysis and model training

## Task a
```{r E2_a_1}
weatherHistory <- read.csv("weatherHistory.csv")
head(weatherHistory)
```
**Qualitative nominal**

* Summary
* Precip.Type
* Daily.Summary

**Quantitative Continuous:**

* Temperature..C.
* Apparent.Temperature..C.
* Humidity
* Wind.Speed..km.h.
* Visibility..km.
* Wind.Bearing..degrees (Reason: Not ranked)

**Quantitative Discrete:**

* Formatted.Date
* Loud.Cover

### Qualitative nominal
``` {r E2_a_2}
weatherHistory %>%
  group_by(Precip.Type) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Precip.Type, y=Count)) +
  geom_bar(stat='identity', position='dodge')
```


``` {r E2_a_3}
weatherHistory %>%
  group_by(Summary) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Summary, y=Count)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle=90))
```
``` {r E2_a_4}
weatherHistory %>%
  group_by(Daily.Summary) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Daily.Summary, y=Count)) +
  geom_bar(stat='identity', position='dodge')
```


### Discrete nominal
```{r E2_a_5}
weatherHistory %>%
  ggplot(aes(Temperature..C.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```


```{r E2_a_6}
weatherHistory %>%
  ggplot(aes(Apparent.Temperature..C.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```


```{r E2_a_7}
weatherHistory %>%
  ggplot(aes(Humidity)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```
```{r E2_a_8}
weatherHistory %>%
  ggplot(aes(Wind.Speed..km.h.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```
```{r E2_a_9}
weatherHistory %>%
  ggplot(aes(Visibility..km.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```

``` {r E2_a_11}
weatherHistory %>%
  ggplot(aes(Wind.Bearing..degrees.)) +
  geom_histogram(aes(y = ..density..), fill = "white", color="black") +
  stat_density(kernel = "gaussian", fill = NA, colour = "black")
```

### Quantative discrete
```{r E2_a_10}
weatherHistory %>%
  group_by(Loud.Cover) %>%
  summarize(Count = n()) %>%
  ggplot(aes(x=Loud.Cover, y=Count)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle=90))
```

## Task b
First removing all columns that seem irrelevant, reasoning:

* Formatted.Date : When encoded it will be equal to row label (1, 2, 3, ...) which tells nothing
* Loud.Cover : All values are 0, therfore tells nothing
* Daily.Summary : Too big to onehotencode effectivly

Then remove all rows with NA, do this after removing irrelevant columns so data is not lost to having NA in the removed columns
```{r E2_b_1}
drop_weatherHistory <- weatherHistory %>% dplyr::select(-c("Formatted.Date", "Daily.Summary", "Loud.Cover"))
drop_weatherHistory <- na.omit(drop_weatherHistory) # Remove all NA
head(drop_weatherHistory)
```

``` {r E2_b_2}

num_wH <- drop_weatherHistory %>%
  dplyr::select(-c("Summary", "Precip.Type")) 
num_stand_wH <- as.data.frame(sapply(num_wH, function(x) ((x-mean(x))/sd(x))))

qualitative_wH <- drop_weatherHistory %>%
  dplyr::select(c("Summary", "Precip.Type")) #Omitted "Formatted.Date","Daily.Summary"

q1 <- table(1:nrow(drop_weatherHistory), drop_weatherHistory$Precip.Type) # as.data.frame.matrix(
q2 <- table(1:nrow(drop_weatherHistory), drop_weatherHistory$Summary)
q <- as.data.frame.matrix(cbind(q1, q2))

cleaned_wH <- cbind(num_stand_wH, q)
head(cleaned_wH)

sample <- sample(c(T, F), nrow(cleaned_wH), replace=T, prob=c(0.75, 0.25))
test_wH <- cleaned_wH[!sample,]
train_wH <- cleaned_wH[sample,]
```
## Task c
Reason for chosen variables:

* Temperate (C) : Baseline that gets moved
* Humidity : Feels a lot hotter when its more humid, harder to sweat
* Wind speed : Wind makes skin feel colder
* Pressure : Pressure changes based on if it may rain or not, feels different
* Rain/Snow : If it rains the air feels colder

```{r E2_c_1}
wH_lm <- train_wH %>%
  lm(Apparent.Temperature..C. ~ rain + snow + Pressure..millibars. + Humidity + Temperature..C. + Wind.Speed..km.h., .)

summary.aov(wH_lm)
summary(wH_lm)
```
As can be seen an the ANOVA and t test for the different values, they are all significant within $\alpha \approx 0$ which means that there is almost 0 chance that the factors are due to random chance. (FIX LATER, DOUBBLE CHECK)
```{r E2_2_vals}
y_test_true <- test_wH$Apparent.Temperature..C.
y_test_pred <- predict(wH_lm, newdata = test_wH)
y_train_true <- train_wH$Apparent.Temperature..C.
y_train_pred <- predict(wH_lm, newdata = train_wH)
```

### RMSE

$$ \text{RMSE}(y, \hat{y}) = \sqrt{\frac{\sum_{i=0}^{N - 1} (y_i - \hat{y}_i)^2}{N}} $$
```{r E2_c_2}
rmse_test <- RMSE(y_test_pred, y_test_true)
rmse_train <- RMSE(y_train_pred, y_train_true)
rmse_test
rmse_test

```
### MAE
$$ \text{MAE}(x,y) = \sum_{i=1}^{D}|x_i-y_i|  $$
```{r E2_c_3}
mae_test <- MAE(y_test_pred, y_test_true)
mae_train <- MAE(y_train_pred, y_train_true)
mae_test
mae_train
```

### $\text{R}^{2}$ score (coefficient of determination)
$$ 
R^2 = 1 - \frac{\text{SSR (sum of square regression)}}{\text{SST (total sum of squares)}} 
= 1 - \frac{\sum_{i=0}^{N - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{N - 1} (y_i - \hat{y})^2}
$$
```{r E2_c_4}
R2_test <- R2(y_test_pred, y_test_true)
R2_train <- R2(y_train_pred, y_train_true)
R2_test
R2_train
```

# Exercise 3 - Linear Regression and Diagnostic Plots

## Task a

### a) Linearity

The relationship between X and Y is linear.

If a function is linear, then all factors increase at a consistent rate for each step.
Gradient and all partial first order derivatives will be a function of constants
Linear function: $y(x,z) = 2x + 4z + 5xz + 10$
Non-Linear function: $y(x,z) = 2x^2 + 5z^5$

### b) Homoscedasticity 

Residuals have constant variance.

homogeneity of variance assumes that all observations are picked from a sources that have equal variance.
In other words the data points around a linear model should vary equally from the line, if there are any cone shape or other such irregularities this assumption is broken and the model that will be produced will eb flawed.

### c) Independence

Residuals are independent.

Observations can't depend on each other, in other words if you pick one observation from a population for your sample, this should not affect the next sample you choose. In other words no observations should depend or affect each other.

By the very nature of lm, it is **assumed** that you have a i.i.d dataset. This has to be done in the sampeling stage, not the cleaning stage.

### d) Normaility

Residuals are normally distributed.

Assumes that the data follows a normal distribution.

Can be tested by making a Q-Q plot and seeing how well the data points follows  the line.
If the right tail is very heavy, you probably should log the value
slightly heavy tails can still be used because of the law of large numbers.


## Task b

These are all diagnostic plots that allow us to test our assumptions

### Residuals vs Fitted

A scatter plot where residuals are on the y-axis and fitted values are on the x-axis. Used to detect non-linearity, unequal error variances, and outliers.
```{r E3_b_1}
plot(wH_lm,1)
```

### Normal Q-Q

Points on the Normal Q-Q plot provide an indication of univariate normality of the dataset.
If the data is normally distributed, the points will fall on the line, otherwise it implies that the assumption of Normality is broken.
To test for normality of residuals
```{r E3_b_2}
plot(wH_lm,2)
```

### Scale-Location (or Spread-Location)

Simmilar to residuals vs fitted, but instead of using residuals on the y-axis it uses the square root of the residuals.
Used to check for the assumption of homoscedascity. If the line is roughly horizontal and there is no clear pattern (like a cone) in the scatter plot then homoscedacity is lokely satisfied.
```{r E3_b_3}
plot(wH_lm,3)
```

### Residuals vs Leverage
Allows us to identify influential observations
**Leverage:** extent to which the coefficients in the regression model would change if a particular observations was removed from the dataset (outliers).
**Standarized residuals:** standardized difference between a predicted value for an observation and the actual value of the observation.
```{r E3_b_4}
plot(wH_lm,5)
```

## Task c
```{r E3_c_setup}
data_gen <- function(c=0, t=0, cm=0, cp=100, cs=0, nnm=1, nnp=10000, snn=500, nnnm=0, nnnp=10000, snnn=500) {
  set.seed(42)
  
  n <- 1000
  x <- 1:n
  
  # Changeable parameters
  # - Change the parameters to affect the generated data points below.
  # - You may copy this code multiple times to answer all the questions in the exercise.
  # - You may find it reasonable to argue for multiple violations from a single generated set of data points.
  
  contant <- c 
  trend <- t
  curve_magnitue <- cm
  curve_period <- cp
  curve_shift <- cs
  normal_noise_magnitue <- nnm
  norm_noise_periode <- nnp
  shift_norm_noise <- snn
  non_normal_noise_magnitue <- nnnm
  non_norm_noice_periode <- nnnp
  shift_non_norm_noise <- snnn
  
  y.gen <- contant +
    trend * x + 
    curve_magnitue* sin(
      (x/curve_period + curve_shift)*pi
      ) + 
    normal_noise_magnitue*cos(
      (x/norm_noise_periode + shift_norm_noise/norm_noise_periode)*pi
      )*rnorm(n, sd = 3) +
    non_normal_noise_magnitue*cos(
      (x/non_norm_noice_periode + shift_non_norm_noise/non_norm_noice_periode)*pi
      ) * rexp(n, rate = 0.2) 
  
  p <- qplot(x, y.gen, ylab = "y") +
    geom_point(size = 0.1) +
    labs(title = "Data generate for linear regrestion")
  
  # Display the plot
  print(p)
  return(list("x"=x,"y.gen"=y.gen))
}
```

### Holds all the assumptions

```{r E3_c_plots}
gen_data <- data_gen()
lm.gen <- lm(gen_data$y.gen ~ gen_data$x)
plot(lm.gen, which = 1)
plot(lm.gen, which = 2)
plot(lm.gen, which = 3)
plot(lm.gen, which = 5)
```

### Breaks the assumption of Homoscedastiicty

```{r E3_c_bHomoscedastiicty}
gen_data <- data_gen(nnnm = -10000, snnn = 5000, nnnp = 50000)
lm.gen <- lm(gen_data$y.gen ~ gen_data$x)
plot(lm.gen, which = 2)
```

### Breaks the assumption of Linearity

```{r E3_c_bLinear}
gen_data <- data_gen(nnnm = -10000, snnn = 5000, cs=10000, cm = 10000, snn = 10000)
lm.gen <- lm(gen_data$y.gen ~ (gen_data$x)^4)
plot(lm.gen, 1)
```

### Breaks the assumption of Normality.
```{r E3_c_bNorm}
gen_data <- data_gen(nnnm = 10000, snnn = 5000, cs=10000, cm = 10000, snn = 10000, nnm = 0, nnnp = 5000, t = 1)
lm.gen <- lm(gen_data$y.gen ~ gen_data$x)
plot(lm.gen, 2)
```

## Task d

### Breaks Homoscedasticity

The error distibution is not consistent, aka $$ \epsilon_i \ne \sigma^2 $$ instead it is
$$ \sigma_{i}^2 = x_{i} \sigma^2 $$, in other words heteroscedastic.

If plot X against Y you will see a cone shape instead of expected line, clearly there are different distributions of variances based on X

### Breaks Linearity

The Residual vs Fitted shows a clear cone as well as a clear non-linear pattern between X and Y

### Breaks Normality

The Q-Q Normal plot clearly shows high bias towards the tails


# Exercise 4 - correlation and partial correlation

## Task a

**Correlation:** Degree to which a pair of variables/factors are linearly related. As in the increase in one variable either increases or decreases another. Correlation does not imply causation, even if Smoking and higher life expectancy is higher in one area, that doesn't automatically imply that smoking is healthy.
The correlation coefficient, typically denoted as ρ, ranges from -1 to 1:
* A value of 1 indicates a perfect positive correlation,
* A value of -1 indicates a perfect negative correlation,
* A value of 0 indicates no correlation.

**Effect of Scaling:**
When variables are scaled by constants α1 and α2, the correlation is unaffected in magnitude but may change sign:
ρW1W2 = cov(α1X, α2Y) / (sqrt(var(α1X)) * sqrt(var(α2Y)))
This simplifies to:
ρW1W2 = sgn(α1α2) * ρXY
Thus, scaling does not change the inherent linear relationship but can alter its direction.



Higher variance decreases correlation, while higher covariance increases it. In other words the more the two variables/factors trend and the less variance from that trend they have the more correlated the two varaibles/factors are.

## Task b

**Partial correlation:** measures the degree of association between two random variables, with the effect of a set of controlling random variables removed. [Wiki](https://en.wikipedia.org/wiki/Partial_correlation)
Helps to understand the relationship between two variables, controlling for the effect of one or more additional variables.

The formula describes the correlation between the residuals $e_X$ and $e_Y$ resulting from the linear regression of X with Z and of Y with Z, respectively.

### Scenarios

1. You suspect there are inter dependencies between X=Study hours per week, Y=Exam score and Z=IQ
2. You suspect there are inter dependencies between X=Calories consumed, Y=Weight loss and Z=Hours of intense exercise.
3. You suspect there are inter dependencies between X=Time spent on social media, Y=Self-esteem and Z=Age


## Task c

Yes, the property of correlation regarding scaling holds for partial correlation as well.
Just like regular correlation, partial correlation is not affected by scaling of the variables, only the direction or sign can change.
Let's do the math so see how this is the case:

1.
In the partial correlation formula we have:
    Pxy
    Pxz
    Pyz

2.
if we then scale the x and y variables with a1 and a2 we get:
    Pxy = sgn(a1,a2)*Pxy
    Pxz = sgn(a1)*Pxz
    Pyz = sgn(a2)*Pyz

3.
Then we substitute the coefficients with the scaled ones and obtain the formula:
Pxy|z = (Pxy - Pxz*Pyz) / (sqrt(1-(Pxz)^2)*sqrt(1-(Pyz)^2) -> (sgn(a1,a2)*Pxy - sgn(a1)*Pxz*sgn(a2)*Pyz) / (sqrt(1-(sgn(a1)*Pxz)^2)*sqrt(1-(sgn(a2)*Pyz)^2)

4.
after doing the math and simplifying the formula we obtain sgn(a1,a2)*Pxy|z
  

## Task d

```{r E4_setup}
weatherHistory <- read.csv("weatherHistory.csv")
select_wH <- weatherHistory %>% dplyr::select(Temperature..C., Apparent.Temperature..C., Humidity)
```

```{r E4_pairwise}
pairwise_corr <- select_wH %>%
  cor() %>%
  data.frame()
pairwise_corr
```

```{r E4_partialCorr}
part_corr <- select_wH %>%
  pcor()

part_corr
```

* Apparent.Temperature..C. and Temperature..C. has a partial correlation value of ca 0.99 which implies that they are highly consistent with each other and increase with each other. Very low difference between pairwise to partial where they are equal if rounded to nearest 2 decimal, which implies that Z=Humidity has very little effect on them.
* Humidity and Temperature..C. has a low partial correlation value of -0.35 which implies that they are not consistent with each other and decreases when the other increases. Decreases from -0.63 to -0.35 which implies that Z=Apparent.Temperature.C.. is affecting them. Z is a significant confounder.
* Humidity and Apparent.Temperature..C. has a low partial correlation value of 0.27 which implies that they are not consistent with each other and increase with each other. Dropped from pairwise correlation -0.60 to 0.27 which implies that Z=Temperature..C. is significantly affecting them. Z is a significant cofunder.
