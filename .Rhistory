print("Error: the data retrieval was not successful!")
stop()
}
} else {
raw_data <- read_rds(weather_file)
print(str_glue("Raw data (JSON) read from '{weather_file}'"))
}
df <- unnest(raw_data$data, cols = c(observations))
df |> dplyr::select(referenceTime, value) |>
mutate(referenceTime=as.Date(referenceTime)) |>
rename(Date=referenceTime, Temp=value) |>
as_tsibble(index = Date) -> dc
if (!is.na(cutoff_date_glob)) {
dc <- dc %>% tsibble::filter_index(cutoff_date_glob ~ .)
}
return(dc)
}
station_list <- c()
for (i in 1:8) {
station_list <- c(station_list, as.character(timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[i])))$Date[1]))
}
station_list
start_date_dc <- max(station_list)
start_date_dc
dc_stations <- timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[1]), start_date_dc)) %>%
rename(!!paste("Temp", cut_stations$name[1], sep=".") := "Temp")
for (i in 2:8) {
dc_stations[paste("Temp", cut_stations$name[i], sep=".")] <- timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[i]), start_date_dc))$Temp
}
head(dc_stations)
dc_stations %>%
subset(select = -c(Date)) %>%
cor() %>%
round(2) %>%
data.frame() %>%
ggcorrplot(type = "upper", lab = TRUE)
p_list <- list()
for (colname in colnames(dc_stations)) {
dc_stl <- stl(ts(dc_stations[[colname]], frequency = 365), s.window=41, t.window=11, robust=TRUE)
pl <- autoplot(dc_stl) +
ggtitle(colname)
p_list[[colname]] <- pl
}
p_list[[2]]
p_list[[3]]
p_list[[4]]
p_list[[5]]
p_list[[6]]
p_list[[7]]
p_list[[8]]
p_list[[9]]
mts_pca <- dc_stations %>%
subset(select = -c(Date)) %>%
prcomp(center = TRUE)
plot(mts_pca, type = "l")
data.frame(date = dc_stations$Date, mts_pca$x) %>%
pivot_longer(!date, names_to="Features", values_to="Scores") %>%
ggplot(aes(x =  date, y = Scores, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
data.frame(date = dc_stations$Date[1:600], mts_pca$x[1:600,]) %>%
pivot_longer(!date, names_to="Features", values_to="Scores") %>%
ggplot(aes(x =  date, y = Scores, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
nrow(mts_pca$x[,1:2])
length(dc_stations$Date)
t <- dc_stations$Date
mts_x_1 <- mts_pca$x[, 1:2] %*% t(mts_pca$rotation[, 1:2])
mts_x_2 <- t(mts_pca$center + mts_pca$scale * t(x_1))
t <- dc_stations$Date
mts_x_1 <- mts_pca$x[, 1:2] %*% t(mts_pca$rotation[, 1:2])
mts_x_2 <- t(mts_pca$center + mts_pca$scale * t(mts_x_1))
data.frame(Date = dc_stations$Date, mts_x_1) %>%
pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  Date, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("mts_pca without accounting for scaling")
data.frame(Date = dc_stations$Date, mts_x_2) %>%
pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  Date, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("mts_pca accounting for scaling")
dc_stations %>%
pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  Date, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("Original data")
# Trend over years
dc_c_ts <- ts(dc_cut_filled_imp_noLeap$Temp, frequency = 365) #365 Leap years removed, no need for 365.25
dc_stl <- stl(dc_c_ts, s.window=7, t.window=35, robust=TRUE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
knitr::opts_chunk$set(echo = TRUE)
options(contrasts = c("contr.sum", "contr.poly"))
require("ggplot2")
require("dplyr")
require("ppcor")
require("caret")
require("tidyr")
require("stringr")
require("lubridate")
require("tsibble")
require("ggfortify")
require("gridExtra")
require("reshape2")
library(ggcorrplot) # Easy cross-corrleation plots
library(imputeTS)   # Time series missing value imputation
library(jsonlite)   # handle JSON data returned by Frost
library(tidyr)      # unpack data from JSON format
library(tidyverse)  # data manipulation with mutate etc, string formatting
library(lubridate)  # process date and time information
library(tsibble)    # special tibbles for time series
library(fpp3)       # autoplot() and gg_season() for time series
library(readr)      # to read the Frost client ID from file
airquality <- read.table("AirQualityUCI.csv", sep=";", dec=",", header= T)
airqual <- airquality %>%
dplyr::select(-c("X", "X.1")) %>%
na.omit() %>%
mutate(timedate = dmy_hms(paste(Date, Time))) %>%
dplyr::select(-c("Time", "Date")) %>%
relocate(timedate) %>%
as_tibble()
summary(airqual)
head(airqual)
airqual %>%
pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
pc <- prcomp(airqual[,-1])
summary(pc)
plot(pc, type = "l")
pc_v <- data.frame(PC = paste0("PC ", ncol(pc$x)),
var_explained = pc$sdev^2 / sum(pc$sdev^2)) %>%
mutate(cum_explained = cumsum(var_explained))
pp <- pc_v[1:3,] %>%
pivot_longer(!PC, names_to="Quantity", values_to="Explained") %>%
ggplot(aes(x = PC, y = Explained, color=Quantity, group=Quantity))+
geom_line() + geom_point() +
theme_minimal() +
labs(title = "Variance Explained", x = "Principal Component",
y = "Variance Explained")
pp + geom_label(aes(label = round(Explained, 2)))
autoplot(pc, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
autoplot(pc, x=2,y=3, loadings=TRUE, loadings.label=TRUE)
pairs(pc$x[,1:3], main = "PCA Score Plots", pch = 19)
airqual_NA <- airqual
airqual_NA[airqual_NA == -200] = NA
missing_values <- colSums(is.na(airqual_NA)) %>% data.frame()
perc_NA_airqal <- (missing_values/nrow(airqual_NA))*100
missing_values
perc_NA_airqal
airqual_NA <- airqual
airqual_NA[airqual_NA == -200] = NA
missing_values <- colSums(is.na(airqual_NA))
print("Missing Values in Each Sensor:")
print(missing_values)
# 2. Investigate the degree of missing values
# Visualizing the number of missing values over time
# Create a data frame to visualize missing values
missing_values <- airqual_NA %>%
mutate(Missing = rowSums(is.na(airqual_NA))) %>%
dplyr::select(timedate, Missing)
ggplot(missing_values, aes(x = timedate, y = Missing)) +
geom_line(color = "red") +
labs(title = "Number of Missing Values Over Time", x = "DateTime", y = "Missing Values Count") +
theme_minimal()
airqual_c <- airqual %>%
dplyr::select(-c("NMHC.GT."))
airqual_c[airqual_c == -200] = NA
airqual_c <- imputeTS::na_ma(airqual_c, k = 4, weighting = "simple")
airqual_c %>%
pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
pca <- prcomp(airqual_c[,-1]) #, center = TRUE, scale. = TRUE
# screeplot
pca <- prcomp(airqual_c[,-1])
plot(pca, type = "l")
# biplots
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=2,y=3, loadings=TRUE, loadings.label=TRUE)
autoplot(pca, x=3,y=4, loadings=TRUE, loadings.label=TRUE)
#Variance explained
summary(pca)$importance[3,]
t <- airqual_c$timedate
x_1 <- pca$x[, 1:6] %*% t(pca$rotation[, 1:6])
x_2 <- t(pca$center + pca$scale * t(x_1))
data.frame(timedate = airqual_c$timedate, x_1) %>%
pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("PCA without accounting for scaling")
data.frame(timedate = airqual_c$timedate, x_2) %>%
pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("PCA accounting for scaling")
airqual_c %>%
pivot_longer(!timedate, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  timedate, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("Original data")
data.frame(timedate = airqual_c$timedate, pca$x[,1:6]) %>%
pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
ggplot(aes(x =  timedate, y = Scores, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
data.frame(timedate = airqual_c$timedate[1:100], pca$x[1:100,1:6]) %>%
pivot_longer(!timedate, names_to="Features", values_to="Scores") %>%
ggplot(aes(x =  timedate, y = Scores, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
autoplot(pca, x=1,y=2, loadings=TRUE, loadings.label=TRUE)
data.frame(pca$rotation)
melt(pca$rotation) %>%
ggplot(aes(Var2, Var1)) +
geom_tile(aes(fill = value)) +
geom_text(aes(fill = value, label = round(value, 3)))
.client_id <- str_trim(read_file("client_id.txt"))
# Server to collect data from and resource we want from server
server <- "frost.met.no"
resource <- "observations/v0.jsonld"
# Station(s) we want data for. SN17850 is the station ID for Ås (Blindern is SN18700)
sources <- 'SN17850'
# Type of data we want, P1D means daily data
elements <- 'mean(air_temperature P1D)'
# Time range we want data for
reference_time <- '1874-01-01/2023-12-31'
# Specify that we want mean temperature calculated from midnight to midnight
timeoffsets <- 'PT0H'
.query_url <- str_glue("https://{.client_id}@{server}/{resource}?sources={sources}&referencetime={reference_time}&elements={elements}&timeoffsets={timeoffsets}")
# Set this to TRUE to generate a json file
get_data_from_frost = TRUE
weather_file = "weather_data_json.rds.bz2"
if ( get_data_from_frost ) {
raw_data <- try(fromJSON(URLencode(.query_url), flatten=TRUE))
if (class(raw_data) != 'try-error') {
print("Data retrieved from frost.met.no!")
write_rds(raw_data, weather_file, compress="bz2", text=TRUE)  # JSON represents data as text
print(str_glue("Raw data (JSON) written to '{weather_file}'"))
} else {
print("Error: the data retrieval was not successful!")
}
} else {
raw_data <- read_rds(weather_file)
print(str_glue("Raw data (JSON) read from '{weather_file}'"))
}
df <- unnest(raw_data$data, cols = c(observations))
head(df)
df |> dplyr::select(referenceTime, value) |>
mutate(referenceTime=as.Date(referenceTime)) |>
rename(Date=referenceTime, Temp=value) |>
as_tsibble(index = Date) -> dc
head(dc)
has_gaps(dc)
gaps <- count_gaps(dc)
head(gaps)
# cutoff_date <-  as.character(tail(gaps[gaps$.n >= 31,], n=1)$.to)
# cutoff_date
if (nrow(gaps) != 0) {
cutoff_date <-  as.character(tail(gaps[gaps$.n >= 31,], n=1)$.to)
dc_cut <- dc %>% tsibble::filter_index(cutoff_date ~ .)
} else {
dc_cut <- dc
}
# dc_cut <- dc %>% tsibble::filter_index(cutoff_date ~ .)
head(dc_cut)
dc_cut_filled <- tsibble::fill_gaps(dc_cut, .full = TRUE)
head(dc_cut_filled)
dc_cut_filled_imp <- imputeTS::na_ma(dc_cut_filled, k = 4, weighting = "simple")
tsibble::has_gaps(dc_cut_filled_imp)
is.numeric(dc_cut_filled_imp$Temp)
dc_cut_filled_imp %>%
ggplot(aes(Date, Temp)) +
geom_line()
dc_cut_filled_imp_noLeap <- dc_cut_filled_imp %>%
dplyr::filter(!(month(Date) == 2 & day(Date) == 29))
head(dc_cut_filled_imp_noLeap)
timeseries_cleaner <- function(table) {
gaps <- count_gaps(table)
gaps_31 <- gaps[gaps$.n >= 31,]
if (nrow(gaps_31) != 0) {
cutoff_date <-  as.character(tail(gaps_31, n=1)$.to)
table_cut <- table %>% tsibble::filter_index(cutoff_date ~ .)
} else {
table_cut <- table
}
table_cut_filled <- tsibble::fill_gaps(table_cut, .full = TRUE)
table_cut_filled_imp <- imputeTS::na_ma(table_cut_filled, k = 4, weighting = "simple")
table_cut_filled_imp_noLeap <- table_cut_filled_imp %>%
dplyr::filter(!(month(Date) == 2 & day(Date) == 29))
return(table_cut_filled_imp_noLeap)
}
identical(timeseries_cleaner(dc), dc_cut_filled_imp_noLeap)
dc %>%
ggplot(aes(Date, Temp)) +
geom_line()
tsibble::fill_gaps(dc, .full = TRUE) %>%
imputeTS::na_ma(airqual_c, k = 4, weighting = "simple") %>%
ggplot(aes(Temp)) +
geom_histogram(aes(y = ..density..), fill = "white", color="black") +
stat_density(kernel = "gaussian", fill = NA, colour = "black") +
ggtitle("imputed values")
dc %>%
ggplot(aes(Temp)) +
geom_histogram(aes(y = ..density..), fill = "white", color="black") +
stat_density(kernel = "gaussian", fill = NA, colour = "black") +
ggtitle("original data")
ts_dc <- ts(dc_cut_filled_imp_noLeap, frequency = 365)
head(ts_dc)
acf(dc$Temp, lag.max = 365*5.5)
acf(dc$Temp, lag.max = 7*4)
months <- c("Janaury", "Febrary")
month_plot <- function(data, date) {
out_plot  <- data %>%
dplyr::filter((month(Date) == date & day(Date) == 1)) %>%
ggplot(aes(Date, Temp)) +
geom_point() +
ylim(-25, 25) +
ggtitle(month.name[i])
return(out_plot)
}
month_plots_list <- list()
for (i in 1:12) {
month_plots_list[[i]] <- month_plot(dc_cut_filled_imp_noLeap, i)
}
gridExtra::grid.arrange(grobs = month_plots_list[1:6], ncol = 3)
gridExtra::grid.arrange(grobs = month_plots_list[7:12], ncol = 3)
# Trend over years
dc_c_ts <- ts(dc_cut_filled_imp_noLeap$Temp, frequency = 365) #365 Leap years removed, no need for 365.25
dc_stl <- stl(dc_c_ts, s.window=7, t.window=35, robust=TRUE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
dc_stl <- stl(dc_c_ts, s.window=7, t.window=100, robust=FALSE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
# STL wide seasonality window
dc_stl <- stl(dc_c_ts, s.window="periodic", t.window=35, robust=TRUE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
# STL wide seasonality window
dc_stl <- stl(dc_c_ts, s.window="periodic", t.window=35, robust=FALSE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
dc_stl <- stl(dc_c_ts, s.window=7, t.window=5, robust=TRUE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
dc_stl <- stl(dc_c_ts, s.window=7, t.window=5, robust=FALSE)
autoplot(dc_stl)
# autocorrelation on remainders
remainders <- dc_stl$time.series[, "remainder"]
acf(remainders, lag.max = 365*5)
acf(remainders, lag.max = 7*4)
.stations_url = str_glue("https://{.client_id}@frost.met.no/sources/v0.jsonld")
raw_stations <- fromJSON(URLencode(.stations_url), flatten=TRUE)
df_stations <- data.frame(raw_stations)
# 2 from Ås SN17850
unique(df_stations[,"data.county"])
COUNTYS = c("AKERSHUS", "FINNMARK", "TROMS")   # replace with names of "fylker" you are interested in
stations <- unnest(raw_stations$data, cols='id') |>
dplyr::select(id, validFrom, country, county, municipality, name, masl, `@type`) |>
mutate(validFrom=as.Date(validFrom)) |>
filter(`@type` == "SensorSystem" & validFrom <= "1950-01-01" & country == "Norge" & county %in% COUNTYS)
cut_stations <- rbind(
stations[stations$county == "AKERSHUS",][1:2,],
stations[stations$county == "TROMS",][c(2,3,5),],
stations[stations$county == "FINNMARK",][c(1,2,4),]
)
cut_stations
get_dc_from_frost <- function(src_id, cutoff_date_glob = NA) {
# Set this to TRUE to generate a json file
weather_file = paste(src_id, "_weather_data_json.rds.bz2", sep="")
get_data_from_frost = !file.exists(file = weather_file)
.query_url <- str_glue("https://{.client_id}@{server}/{resource}?sources={src_id}&referencetime={reference_time}&elements={elements}&timeoffsets={timeoffsets}")
raw_data <- list()
if ( get_data_from_frost ) {
raw_data <- try(fromJSON(URLencode(.query_url), flatten=TRUE))
if (class(raw_data) != 'try-error') {
print("Data retrieved from frost.met.no!")
write_rds(raw_data, weather_file, compress="bz2", text=TRUE)  # JSON represents data as text
print(str_glue("Raw data (JSON) written to '{weather_file}'"))
} else {
print("Error: the data retrieval was not successful!")
stop()
}
} else {
raw_data <- read_rds(weather_file)
print(str_glue("Raw data (JSON) read from '{weather_file}'"))
}
df <- unnest(raw_data$data, cols = c(observations))
df |> dplyr::select(referenceTime, value) |>
mutate(referenceTime=as.Date(referenceTime)) |>
rename(Date=referenceTime, Temp=value) |>
as_tsibble(index = Date) -> dc
if (!is.na(cutoff_date_glob)) {
dc <- dc %>% tsibble::filter_index(cutoff_date_glob ~ .)
}
return(dc)
}
station_list <- c()
for (i in 1:8) {
station_list <- c(station_list, as.character(timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[i])))$Date[1]))
}
station_list
start_date_dc <- max(station_list)
start_date_dc
dc_stations <- timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[1]), start_date_dc)) %>%
rename(!!paste("Temp", cut_stations$name[1], sep=".") := "Temp")
for (i in 2:8) {
dc_stations[paste("Temp", cut_stations$name[i], sep=".")] <- timeseries_cleaner(get_dc_from_frost(as.character(cut_stations$id[i]), start_date_dc))$Temp
}
head(dc_stations)
dc_stations %>%
subset(select = -c(Date)) %>%
cor() %>%
round(2) %>%
data.frame() %>%
ggcorrplot(type = "upper", lab = TRUE)
p_list <- list()
for (colname in colnames(dc_stations)) {
dc_stl <- stl(ts(dc_stations[[colname]], frequency = 365), s.window=41, t.window=11, robust=TRUE)
pl <- autoplot(dc_stl) +
ggtitle(colname)
p_list[[colname]] <- pl
}
p_list[[2]]
p_list[[3]]
p_list[[4]]
p_list[[5]]
p_list[[6]]
p_list[[7]]
p_list[[8]]
p_list[[9]]
mts_pca <- dc_stations %>%
subset(select = -c(Date)) %>%
prcomp(center = TRUE)
plot(mts_pca, type = "l")
data.frame(date = dc_stations$Date, mts_pca$x) %>%
pivot_longer(!date, names_to="Features", values_to="Scores") %>%
ggplot(aes(x =  date, y = Scores, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
data.frame(date = dc_stations$Date[1:600], mts_pca$x[1:600,]) %>%
pivot_longer(!date, names_to="Features", values_to="Scores") %>%
ggplot(aes(x =  date, y = Scores, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3)
t <- dc_stations$Date
mts_x_1 <- mts_pca$x[, 1:2] %*% t(mts_pca$rotation[, 1:2])
mts_x_2 <- t(mts_pca$center + mts_pca$scale * t(mts_x_1))
data.frame(Date = dc_stations$Date, mts_x_1) %>%
pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  Date, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("mts_pca without accounting for scaling")
data.frame(Date = dc_stations$Date, mts_x_2) %>%
pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  Date, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("mts_pca accounting for scaling")
dc_stations %>%
pivot_longer(!Date, names_to="Features", values_to="Measurement") %>%
ggplot(aes(x =  Date, y = Measurement, col = Features)) +
geom_line() +
theme_minimal() +
facet_wrap(~ Features, scales = "free_y", ncol = 3) +
ggtitle("Original data")
