library(tidyverse)
library(readr)
library(lubridate)
library(readxl)
library(imputeTS) # For time series imputation
library(zoo) # for time series manipulation
library(tseries)  # For stationarity tests
library(forecast) # For ARIMA modeling and forecasting
library(stlplus) # An enhanced version of STL
library(lmtest) # For grangertest
# Part 1:
# Task A)
# Import power consumption data
consumption_aas_hour <- read_csv2("consumption_per_group_aas_hour.csv",
locale = locale(tz = "UTC"))
# Select relevant columns and rename STARTTID to DATO for consistency
consumption_aas_hour <- consumption_aas_hour %>%
select(DATO = STARTTID, FORBRUKSGRUPPE, VOLUM_KWH)
# Find contiguous data start
# Convert DATO to POSIXct (datetime) objects with UTC timezone
consumption_aas_hour$DATO <- as.POSIXct(consumption_aas_hour$DATO, tz = "UTC")
# Find the first date with non-NA data
contiguous_start <- min(consumption_aas_hour$DATO[!is.na(consumption_aas_hour$VOLUM_KWH)])
# Filter to keep only contiguous data.
consumption_aas_hour <- consumption_aas_hour %>%
filter(DATO >= contiguous_start)
# Aggregate to daily sums for each consumer group
consumption_aas_day <- consumption_aas_hour %>%
group_by(DATO = as.Date(DATO), FORBRUKSGRUPPE) %>% # Group by date and consumer group
summarize(VOLUM_KWH = sum(VOLUM_KWH)) %>%  # Sum kWh for each group/day
ungroup()
# Task B)
# Import temperature and irradiation data for each year
# Function to import and format data from a single year
import_weather_data <- function(year) {
filename <- paste0("Aas dogn ", year, ".xlsx")
# Read the Excel file, handling potential date format issues
tryCatch({  # Try a direct read with date parsing
data <- read_excel(filename)
}, error = function(e) { # If date parsing fails, read as text then convert
data <- read_excel(filename, col_types = "text")
data$DATO <- as.Date(data$DATO, format = "%d/%m/%Y") # Adjust format if needed
})
#Handle different date formats
if(year == 2024){
data$DATO <- as.Date(data$DATO, origin = "1899-12-30")
}else if(year %in% c(2017, 2018, 2022, 2023)){
data$DATO <- as.Date(data$DATO, format = "%d/%m/%Y") # For 2017, 2018, 2022, 2023
}else{ # For years 2019, 2020, 2021
if(!is.Date(data$DATO)) data$DATO <- as.Date(data$DATO, origin = "1899-12-30") # Leap year + Excel dates
}
data <- data %>%
select(DATO, LT, GLOB)
# Impute missing values using linear interpolation
# Imputation should be done carefully considering the nature of the data and
# potential seasonal patterns.
data$LT <- na_interpolation(data$LT, option = "linear")
data$GLOB <- na_interpolation(data$GLOB, option = "linear")
return(data)
}
# Import and combine data for all years
weather_data <- map_dfr(2017:2024, import_weather_data)
# Task C)
# Find the range of dates for each dataset
consumption_date_range <- range(consumption_aas_day$DATO)
weather_date_range <- range(weather_data$DATO)
# Find the overlapping date range
overlapping_start <- max(consumption_date_range[1], weather_date_range[1])
overlapping_end <- min(consumption_date_range[2], weather_date_range[2])
# Filter and merge the datasets based on the overlapping range
merged_data <- consumption_aas_day %>%
filter(DATO >= overlapping_start, DATO <= overlapping_end) %>%
left_join(weather_data %>% filter(DATO >= overlapping_start, DATO <= overlapping_end), by = "DATO")
# Remove leap days
merged_data <- merged_data %>%
filter(!(month(DATO) == 2 & day(DATO) == 29))
# Merged data is ready for further analysis/forecasting.
# Part 2: Forecasting Analysis
# --- Task D: Exploratory Data Analysis ---
# 1. Data Visualization
merged_data %>%
pivot_longer(cols = c(VOLUM_KWH, LT, GLOB), names_to = "Measurement", values_to = "Value") %>%
ggplot(aes(x = DATO, y = Value, color = FORBRUKSGRUPPE)) +
geom_line() +
facet_wrap(~Measurement, scales = "free_y") +  # Separate panels for each measurement
theme_bw() +
labs(title = "Raw Data Visualization", x = "Date", y = "Value") +
theme(legend.position = "bottom") # Move legend to avoid overlap
# 2. Stationarity Tests
consumer_groups <- unique(merged_data$FORBRUKSGRUPPE)
test_stationarity <- function(data, group_name) {
print(paste("Stationarity Test for", group_name))
# Handle NAs (Important: Use na.remove *inside* the function)
data <- na.remove(data)
adf.test(data) %>% print()  # Augmented Dickey-Fuller Test
kpss.test(data) %>% print()  # KPSS Test
cat("\n") # Newline for better output formatting
}
# Stationarity for consumption (by consumer group)
for (group in consumer_groups) {
ts_data <- ts(merged_data$VOLUM_KWH[merged_data$FORBRUKSGRUPPE == group], frequency = 365)
test_stationarity(ts_data, group)
}
# Stationarity for temperature and global irradiation
ts_temp <- ts(merged_data$LT, frequency = 365)
test_stationarity(ts_temp, "Temperature (LT)")
ts_glob <- ts(merged_data$GLOB, frequency = 365)
test_stationarity(ts_glob, "Global Irradiation (GLOB)")
# 3. Cross-Correlation Analysis
# Calculate and display the cross-correlation matrix
cor_matrix <- cor(merged_data %>% select(where(is.numeric)), use = "pairwise.complete.obs")
print(cor_matrix)
# Robustly handle NAs within ccf using na.action = na.contiguous:
numeric_cols <- names(merged_data)[sapply(merged_data, is.numeric) & names(merged_data) != "DATO"]
for(i in 1:(length(numeric_cols) - 1)) {
for(j in (i + 1):length(numeric_cols)) {
# Create time series objects
ts1 <- ts(merged_data[[numeric_cols[i]]], frequency = 365)
ts2 <- ts(merged_data[[numeric_cols[j]]], frequency = 365)
ccf_result <- ccf(ts1, ts2,
lag.max = 30,
na.action = na.contiguous,  # Handle NAs correctly within ccf
main = paste0("Cross-Correlation: ", numeric_cols[i], " vs. ", numeric_cols[j]))
print(ccf_result)
plot(ccf_result) # Plot the results
}
}
# 4. Autocorrelation (ACF) and Partial Autocorrelation (PACF) Functions
#ACF/PACF functions help you understand time dependencies, autoregressive and moving average components of time series.
# --- Consumption (by consumer group) ---
for (group in consumer_groups) {
consumption_data <- merged_data$VOLUM_KWH[merged_data$FORBRUKSGRUPPE == group]
# Create the time series *after* removing NAs:
ts_data <- ts(na.remove(consumption_data), frequency = 365)  # Remove NAs *before* ts()
acf(ts_data, lag.max = 14, main = paste("ACF:", group,  "(Short Term)"))
pacf(ts_data, lag.max = 14, main = paste("PACF:", group,  "(Short Term)"))
acf(ts_data, lag.max = 365*2, main = paste("ACF:", group, "(Long Term)"))
pacf(ts_data, lag.max = 365*2, main = paste("PACF:", group, "(Long Term)"))
}
# --- Temperature ---
# Remove NAs *before* creating time series
ts_temp <- ts(na.remove(merged_data$LT), frequency = 365)  # Correct: remove NAs first
acf(ts_temp, lag.max = 14, main = "ACF: Temperature (LT) (Short Term)")
pacf(ts_temp, lag.max = 14, main = "PACF: Temperature (LT) (Short Term)")
acf(ts_temp, lag.max = 365*2, main = "ACF: Temperature (LT) (Long Term)")
pacf(ts_temp, lag.max = 365*2, main = "PACF: Temperature (LT) (Long Term)")
# --- Global Irradiation ---
# Remove NAs *before* creating time series
ts_glob <- ts(na.remove(merged_data$GLOB), frequency = 365) # Correct: remove NAs first
acf(ts_glob, lag.max = 14, main = "ACF: Global Irradiation (GLOB) (Short Term)")
pacf(ts_glob, lag.max = 14, main = "PACF: Global Irradiation (GLOB) (Short Term)")
acf(ts_glob, lag.max = 365*2, main = "ACF: Global Irradiation (GLOB) (Long Term)")
pacf(ts_glob, lag.max = 365*2, main = "PACF: Global Irradiation (GLOB) (Long Term)")
# Task E) Seasonal Differencing and ACF/PACF
# --- Seasonal Differencing ---
# Consumption (by consumer group)
for (group in consumer_groups) {
consumption_data <- merged_data$VOLUM_KWH[merged_data$FORBRUKSGRUPPE == group]
ts_data <- ts(na.remove(consumption_data), frequency = 365)
diff_consumption <- diff(ts_data, lag = 365) # Seasonal differencing (lag = 365 for yearly)
# ACF and PACF after differencing (short and long term)
acf(diff_consumption, lag.max = 14, main = paste("ACF:", group, "(Seasonally Differenced, Short Term)"))
pacf(diff_consumption, lag.max = 14, main = paste("PACF:", group, "(Seasonally Differenced, Short Term)"))
acf(diff_consumption, lag.max = 365*2, main = paste("ACF:", group, "(Seasonally Differenced, Long Term)"))
pacf(diff_consumption, lag.max = 365*2, main = paste("PACF:", group, "(Seasonally Differenced, Long Term)"))
}
# Temperature
ts_temp <- ts(na.remove(merged_data$LT), frequency = 365)
diff_temp <- diff(ts_temp, lag = 365)
acf(diff_temp, lag.max = 14, main = "ACF: Temperature (Seasonally Differenced, Short Term)")
pacf(diff_temp, lag.max = 14, main = "PACF: Temperature (Seasonally Differenced, Short Term)")
acf(diff_temp, lag.max = 365 * 2, main = "ACF: Temperature (Seasonally Differenced, Long Term)")
pacf(diff_temp, lag.max = 365 * 2, main = "PACF: Temperature (Seasonally Differenced, Long Term)")
# Global Irradiation
ts_glob <- ts(na.remove(merged_data$GLOB), frequency = 365) # Remove NAs
diff_glob <- diff(ts_glob, lag = 365)
acf(diff_glob, lag.max = 14, main = "ACF: Global Irradiation (Seasonally Differenced, Short Term)")
pacf(diff_glob, lag.max = 14, main = "PACF: Global Irradiation (Seasonally Differenced, Short Term)")
acf(diff_glob, lag.max = 365*2, main = "ACF: Global Irradiation (Seasonally Differenced, Long Term)")
pacf(diff_glob, lag.max = 365*2, main = "PACF: Global Irradiation (Seasonally Differenced, Long Term)")
# --- Task F: STL Decomposition Analysis ---
# Choose STL parameters
stl_parameters <- list(s.window = "periodic",  # Seasonal window for yearly data
t.window = 365,         # Trend window (yearly smoothing)
l.window = 30,          # Low-frequency window (adjust as needed)
robust = TRUE)         # Use robust STL (handles outliers better)
ma_order <- 7 # Set the order for the moving average smoother (e.g., 7 for weekly smoothing).  You can adjust this based on the data. A larger order leads to more smoothing.
# --- Private Electricity Consumption ---
for (group in consumer_groups[consumer_groups == "Privat"]) {
consumption_data <- merged_data$VOLUM_KWH[merged_data$FORBRUKSGRUPPE == group]
ts_consumption <- ts(na.remove(consumption_data), frequency = 365)
# Perform STL decomposition
stl_consumption <- stlplus(ts_consumption, period = 365,
s.window = stl_parameters$s.window,
t.window = stl_parameters$t.window,
l.window = stl_parameters$l.window,
robust = stl_parameters$robust)
# Smooth the trend-cycle (AFTER STL) using a moving average
smoothed_trendcycle <- ma(stl_consumption$data[, "trend"], order = ma_order) # Using moving average.
# Extract components
seasonal_consumption <- stl_consumption$data[, "seasonal"] # Seasonal component (periodic fluctuations)
remainder_consumption <- stl_consumption$data[, "remainder"]  # Remainder component (noise or error)
trendcycle_consumption <- stl_consumption$data[, "trend"]
if ("cycle" %in% colnames(stl_consumption$data)) {
trendcycle_consumption <- trendcycle_consumption + stl_consumption$data[, "cycle"]
}
deseasoned_consumption <- ts_consumption - seasonal_consumption  # Deseasonalized data
# Plot components
plot(ts_consumption, main = "Private Electricity Consumption") # Original
plot(seasonal_consumption, main = "Seasonal Component")      # Seasonal
plot(deseasoned_consumption, main = "Deseasoned Consumption")  # Deseasoned
plot(smoothed_trendcycle, main = "Smoothed Trend-Cycle")    # Smoothed trend-cycle
plot(remainder_consumption, main = "Remainder")           # Remainder
}
# --- Global Irradiation ---
ts_glob <- ts(na.remove(merged_data$GLOB), frequency = 365)
# Perform STL Decomposition
stl_glob <- stlplus(ts_glob, period = 365,
s.window = stl_parameters$s.window,
t.window = stl_parameters$t.window,
l.window = stl_parameters$l.window,
robust = stl_parameters$robust)
# Smooth the trend-cycle (using moving average, same order)
smoothed_trendcycle_glob <- ma(stl_glob$data[, "trend"], order = ma_order)
# Extract components (same approach as consumption)
seasonal_glob <- stl_glob$data[, "seasonal"]
deseasoned_glob <- ts_glob - seasonal_glob
trendcycle_glob <- stl_glob$data[, "trend"]
if ("cycle" %in% colnames(stl_glob$data)) {
trendcycle_glob <- trendcycle_glob + stl_glob$data[, "cycle"]
}
remainder_glob <- stl_glob$data[, "remainder"]
#Plot components
plot(ts_glob, main = "Global Irradiation")
plot(seasonal_glob, main = "Seasonal Component")
plot(deseasoned_glob, main = "Deseasoned Irradiation")
plot(smoothed_trendcycle_glob, main = "Smoothed Trend-Cycle") # Plot smoothed trend
plot(remainder_glob, main = "Remainder")
# --- Air Temperature ---
# ... (same structure as Global Irradiation) ...
ts_temp <- ts(na.remove(merged_data$LT), frequency = 365)
# Perform STL Decomposition
stl_temp <- stlplus(ts_temp, period = 365,
s.window = stl_parameters$s.window,
t.window = stl_parameters$t.window,
l.window = stl_parameters$l.window,
robust = stl_parameters$robust)
# Smooth the trend-cycle (using moving average, same order as others)
smoothed_trendcycle_temp <- ma(stl_temp$data[, "trend"], order = ma_order)
seasonal_temp <- stl_temp$data[, "seasonal"]
deseasoned_temp <- ts_temp - seasonal_temp
trendcycle_temp <- stl_temp$data[, "trend"]
if ("cycle" %in% colnames(stl_temp$data)) {
trendcycle_temp <- trendcycle_temp + stl_temp$data[, "cycle"]
}
remainder_temp <- stl_temp$data[, "remainder"]
plot(ts_temp, main = "Air Temperature")
plot(seasonal_temp, main = "Seasonal Component")
plot(deseasoned_temp, main = "Deseasoned Temperature")
plot(smoothed_trendcycle_temp, main = "Smoothed Trend-Cycle") # Plot smoothed trend
plot(remainder_temp, main = "Remainder")
# --- Task G: Granger Causality Test ---
# 1. Hypotheses
# Null Hypothesis (H0):  x does NOT Granger-cause y
#   Past values of x do not help in predicting y.
# Alternative Hypothesis (H1): x DOES Granger-cause y
#  Past values of x provide statistically significant information about the
#  future values of y.
# 2. Test Procedure (using AR and VAR models)
# a. Univariate Autoregression (AR) Model for y:
#   y_t = c + ϕ_1*y_(t-1) + ϕ_2*y_(t-2) + ... + ϕ_p*y_(t-p) + ε_t
#   Model y only using its own past values (lags).
# b. Vector Autoregression (VAR) Model for x and y:
#   y_t = c + ϕ_1*y_(t-1) + ... + ϕ_p*y_(t-p) + β_1*x_(t-1) + ... + β_p*x_(t-p) + ε_t
#   Model y using both its own past values AND the past values of x.
# c. Comparison:
#   Use an F-test to compare the AR and VAR models.
#   If the VAR model (with x) significantly improves prediction of y compared
#   to the AR model (only y's history), reject the null hypothesis.  The improvement
#   in predictive ability is assessed by comparing residual sums of squares
#   between restricted (AR) and unrestricted (VAR) models.
# 3. Applying the Test (Original Data)
# --- Task G: Granger Causality Test ---
# Granger Causality test function
granger_test <- function(y, x, group_name, var_name, maxlag = 5) {
# Align time series and handle potential NA/length issues:
# Find the maximum start date and minimum end date between the two time series
start_date <- max(start(y), start(x))
end_date <- min(end(y), end(x))
# If the time series do not overlap, skip the test and show a warning
if (start_date > end_date) {
warning(paste("No overlapping dates for", group_name, "and", var_name))
return(NULL)
}
# Window the time series to the overlapping date range
y <- window(y, start = start_date, end = end_date)
x <- window(x, start = start_date, end = end_date)
# CRITICAL CHECK: Ensure time series have enough observations after windowing.
# The Granger test needs at least 2 observations to perform the test.
if (length(na.remove(y)) <= 1 || length(na.remove(x)) <= 1) {
warning(paste("Time series too short for Granger test:", group_name, "or", var_name))
return(NULL)
}
# Combine the two time series into a data frame for the Granger test
combined_data <- data.frame(y = y, x = x)
# Display a message indicating the variables being tested
print(paste("Granger Causality Test:", var_name, "->", group_name))
# Perform the Granger Causality Test using the grangertest() function from the lmtest package
test_result <- tryCatch({
# Run the Granger Causality Test with a specified maximum lag (default is 5)
granger_result <- grangertest(y ~ x, order = maxlag, data = combined_data)
return(granger_result)
}, error = function(e) {
# If an error occurs (e.g., due to data issues), catch and display it
cat("Error in Granger test:", e, "\n")
return(NULL)
})
# Print the results of the Granger Causality test if successful
if (!is.null(test_result)) {
print(test_result)
} else {
# If no result, show a message indicating no result was generated
print(paste("No result for", var_name, "->", group_name))
}
# Return the test result (if any)
return(test_result)
}
# --- Example: Granger Causality Test between Consumption and Temperature ---
# Loop through each consumer group to apply the Granger Causality test
for (group in consumer_groups) {
# Extract the consumption data for the specific group (e.g., 'Privat')
consumption_data <- merged_data$VOLUM_KWH[merged_data$FORBRUKSGRUPPE == group]
ts_consumption <- ts(na.remove(consumption_data), frequency = 365)  # Convert to time series with daily frequency
# Extract temperature (LT) data, assuming it is available in merged_data
ts_temp <- ts(na.remove(merged_data$LT), frequency = 365)  # Convert to time series with daily frequency
# Perform the Granger Causality test for Consumption vs Temperature
granger_result <- granger_test(ts_consumption, ts_temp, group, "Temperature (LT)")
# Extract Global Irradiation (GLOB) data
ts_glob <- ts(na.remove(merged_data$GLOB), frequency = 365)
# Perform the Granger Causality test for Consumption vs Global Irradiation
granger_result <- granger_test(ts_consumption, ts_glob, group, "Global Irradiation (GLOB)")
# Perform the Granger Causality test for Temperature vs Global Irradiation
granger_result <- granger_test(ts_temp, ts_glob, group, "Temperature (LT)")
}
# --- Task H: Forecasting with ARIMA Model ---
# ARIMA Model Forecasting function
arima_forecast <- function(time_series, max_order = 5, h = 10) {
# Ensure the time series is a univariate time series (vector)
if (is.null(time_series) || length(time_series) < 2) {
stop("Time series is too short for forecasting")
}
# Check for missing data and handle it
time_series <- na.remove(time_series)  # Remove NA values from the series
# Fit an ARIMA model with automatic order selection
# auto.arima function automatically selects the best ARIMA model based on AICc criterion
model <- auto.arima(time_series, max.p = max_order, max.q = max_order, seasonal = FALSE)
# Display the selected ARIMA model
print(paste("Fitted ARIMA Model:", as.character(model)))
# Forecast the next 'h' periods (e.g., 10 days)
forecast_result <- forecast(model, h = h)
# Plot the forecasted values along with the historical data
plot(forecast_result)
title(main = paste("ARIMA Forecast for next", h, "periods"))
# Return the forecast results (point forecast, lower and upper prediction intervals)
return(forecast_result)
}
# --- Example: Forecasting Consumption for a Specific Group (Privat) ---
# Let's forecast the consumption for the 'Privat' group over the next 10 periods
# Extract consumption data for the 'Privat' group from the merged dataset
consumption_data <- merged_data$VOLUM_KWH[merged_data$FORBRUKSGRUPPE == "Privat"]
# Convert consumption data to a time series object, handling NAs
ts_consumption <- ts(na.remove(consumption_data), frequency = 365)  # Daily frequency (365 days per year)
# Apply ARIMA model forecasting for the next 10 periods (e.g., days or time units)
forecast_result <- arima_forecast(ts_consumption, max_order = 5, h = 10)
# --- Example: Forecasting Temperature (LT) ---
# Now, let's forecast the temperature (LT) for the next 10 periods
# Extract temperature data from merged dataset
temperature_data <- merged_data$LT
# Convert temperature data to a time series object
ts_temperature <- ts(na.remove(temperature_data), frequency = 365)
# Apply ARIMA model forecasting for the next 10 periods (e.g., days or time units)
forecast_result_temp <- arima_forecast(ts_temperature, max_order = 5, h = 10)
# --- Example: Forecasting Global Irradiation (GLOB) ---
# Similarly, forecast Global Irradiation (GLOB) for the next 10 periods
# Extract global irradiation data from merged dataset
glob_data <- merged_data$GLOB
# Convert global irradiation data to a time series object
ts_glob <- ts(na.remove(glob_data), frequency = 365)
# Apply ARIMA model forecasting for the next 10 periods (e.g., days or time units)
forecast_result_glob <- arima_forecast(ts_glob, max_order = 5, h = 10)
# --- Task I: Evaluate Forecast Accuracy for ARIMA Model ---
# Evaluation function to calculate forecast accuracy
evaluate_forecast_accuracy <- function(forecast_result, actual_data) {
# Ensure that both forecast and actual data are provided and have the same length
if (length(forecast_result$mean) != length(actual_data)) {
stop("The forecast and actual data must have the same length")
}
# Calculate accuracy measures: MAE, MSE, RMSE, MAPE, etc.
accuracy_measures <- accuracy(forecast_result, actual_data)
# Print out the accuracy measures
print("Accuracy Measures:")
print(accuracy_measures)
# Return the accuracy measures for further use
return(accuracy_measures)
}
# --- Example 1: Evaluating Forecast Accuracy for Consumption (VOLUM_KWH) ---
# Get the actual consumption data for the last 10 periods
# Assuming that actual future consumption data is available for comparison
actual_consumption <- merged_data$VOLUM_KWH[(length(merged_data$VOLUM_KWH)-9):length(merged_data$VOLUM_KWH)]
# Compare with the forecasted consumption
# Use the forecasted result from Task H
forecast_accuracy_consumption <- evaluate_forecast_accuracy(forecast_result, actual_consumption)
# --- Example 2: Evaluating Forecast Accuracy for Temperature (LT) ---
# Get the actual temperature data for the last 10 periods
actual_temperature <- merged_data$LT[(length(merged_data$LT)-9):length(merged_data$LT)]
# Compare with the forecasted temperature
# Use the forecasted result from Task H
forecast_accuracy_temperature <- evaluate_forecast_accuracy(forecast_result_temp, actual_temperature)
# --- Example 3: Evaluating Forecast Accuracy for Global Irradiation (GLOB) ---
# Get the actual global irradiation data for the last 10 periods
actual_glob <- merged_data$GLOB[(length(merged_data$GLOB)-9):length(merged_data$GLOB)]
# Compare with the forecasted global irradiation
# Use the forecasted result from Task H
forecast_accuracy_glob <- evaluate_forecast_accuracy(forecast_result_glob, actual_glob)
